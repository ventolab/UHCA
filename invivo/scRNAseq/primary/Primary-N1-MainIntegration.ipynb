{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration of biopsies and organ donors from 2 studies\n",
    "import anndata, numpy as np, pandas as pd, lpy, scanpy as sc, wget, imp, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35;46;1mCreate AnnData Object\u001b[0m\u001b[34m\n",
      "def createAnnData(folderlist, prefix, souporcell_folderlist = None, souporcell_genodico = None, autoinclude=[\"percent_mito\", \"log2p1_count\", \"n_genes\"], min_cell_per_gene_allowed=3, min_gene_per_cell_allowed=500, sample_obskey = \"sample_names\",doqcplots=False, doinspect=False, mitogeneprefix=\"MT-\", do_log2_normalize=True):\n",
      "    if doinspect is True: print(\"\\033[35;46;1mCreate AnnData Object\\033[0m\\033[34m\"); print(inspect.getsource(createAnnData));print(\"\\033[31;43;1mExecution:\\033[0m\")\n",
      "    adatas = []\n",
      "    def pickname(x,y):\n",
      "        if (y == \"singlet\"): return(\"_genotype_\" +str(x))\n",
      "        else: return(\"_doublet\")\n",
      "    def pickname2(x,y,z):\n",
      "        if (y == \"singlet\"): return(z[int(x)])\n",
      "        else: return(\"doublet\")\n",
      "    for i in range(len(folderlist)):\n",
      "        print(\"Processing \" + prefix[i])\n",
      "        adatas.append(sc.read_10x_mtx(folderlist[i]))\n",
      "        if souporcell_folderlist is not None:\n",
      "            try:\n",
      "                res = pd.read_csv(souporcell_folderlist[i] + \"clusters.tsv\",sep='\\t')\n",
      "                if (res.shape[0] != len(adatas[i].obs_names)): raise ValueError( prefix[i] + \" data and soupx barcodes mismatches!\")\n",
      "                adatas[i].obs[\"souporcell\"] = res.status.to_list()\n",
      "                if (souporcell_genodico is None) or (prefix[i] not in souporcell_genodico.keys()):\n",
      "                    adatas[i].obs[\"demultiplexed\"] = [prefix[i] + pickname(x,y)   for x,y in zip( res.assignment.to_list() , res.status.to_list()) ]\n",
      "                else:\n",
      "                    adatas[i].obs[\"demultiplexed\"] = [pickname2(x,y, souporcell_genodico[prefix[i]] )   for x,y in zip( res.assignment.to_list() , res.status.to_list()) ]\n",
      "            except:\n",
      "                print(\"No valid genotyping data for \"+ str(len(adatas[i].obs_names)) + \" cells within \" + prefix[i] + \"! Setting everything to singlets\")\n",
      "                adatas[i].obs[\"souporcell\"] = \"singlet\"\n",
      "                if (souporcell_genodico is None):\n",
      "                    adatas[i].obs[\"demultiplexed\"] = prefix[i] + \"_genotype_0\"\n",
      "                else:\n",
      "                    adatas[i].obs[\"demultiplexed\"] = souporcell_genodico[prefix[i]][0]\n",
      "                    \n",
      "        adatas[i].obs[sample_obskey] = [prefix[i] for hehe in adatas[i].obs_names]\n",
      "        adatas[i].obs_names = [prefix[i] + \"_\" + x for x in adatas[i].obs_names]\n",
      "        if \"n_count\" in autoinclude:\n",
      "            adatas[i].obs['n_count'] = np.sum(adatas[i].X, axis=1).A1\n",
      "        if \"log2p1_count\" in autoinclude:\n",
      "            adatas[i].obs['log2p1_count'] = np.log1p(np.sum(adatas[i].X, axis=1).A1) / math.log(2)\n",
      "        if \"percent_mito\" in autoinclude:\n",
      "            mito_genes = [name for name in adatas[i].var_names if name.startswith(mitogeneprefix)]\n",
      "            adatas[i].obs['percent_mito'] = np.sum(adatas[i][:, mito_genes].X, axis=1).A1 / np.sum(adatas[i].X, axis=1).A1\n",
      "        if \"n_genes\" in autoinclude:\n",
      "            adatas[i].obs['n_genes'] = np.sum(adatas[i].X != 0, axis=1).A1\n",
      "        adatas[i].obs[sample_obskey].value_counts()\n",
      "    adata = mergeAnnData(adatas,index_unique = None)\n",
      "    if min_gene_per_cell_allowed is not None: sc.pp.filter_cells(adata, min_genes=min_gene_per_cell_allowed)\n",
      "    if min_cell_per_gene_allowed is not None: sc.pp.filter_genes(adata, min_cells=min_cell_per_gene_allowed)\n",
      "    \n",
      "    if doqcplots is True:\n",
      "        if \"log2p1_count\" in autoinclude:\n",
      "            sc.pl.violin(adata, ['log2p1_count'], jitter=0.4, groupby=sample_obskey, rotation=90)\n",
      "        if \"n_genes\" in autoinclude:\n",
      "            sc.pl.violin(adata, ['n_genes'], jitter=0.4, groupby=sample_obskey, rotation=90)\n",
      "        if \"percent_mito\" in autoinclude:\n",
      "            sc.pl.violin(adata, ['percent_mito'], jitter=0.4, groupby=sample_obskey, rotation=90)\n",
      "    if do_log2_normalize:\n",
      "        adata.raw = adata.copy()\n",
      "        adata.X = np.log1p(adata.X) / math.log(2)\n",
      "    if \"log2p1_count\" in autoinclude:\n",
      "        adata.obs[\"log2p1_count\"] = pd.to_numeric(adata.obs[\"log2p1_count\"], downcast='float')\n",
      "    if \"percent_mito\" in autoinclude:\n",
      "        adata.obs[\"percent_mito\"] = pd.to_numeric(adata.obs[\"percent_mito\"], downcast='float')\n",
      "    return adata;\n",
      "\n",
      "\u001b[31;43;1mExecution:\u001b[0m\n",
      "Processing 4861STDY7387181\n",
      "Processing 4861STDY7387182\n",
      "Processing 4861STDY7387183\n",
      "Processing 4861STDY7771115\n",
      "Processing 4861STDY7771123\n",
      "Processing MRC_Endo8625698\n",
      "Processing MRC_Endo8625699\n",
      "Processing MRC_Endo8712024\n",
      "Processing MRC_Endo8712032\n",
      "Processing MRC_Endo8715415\n",
      "Processing MRC_Endo8715416\n",
      "Processing GSM4577306\n",
      "Processing GSM4577307\n",
      "Processing GSM4577308\n",
      "Processing GSM4577309\n",
      "Processing GSM4577310\n",
      "Processing GSM4577311\n",
      "Processing GSM4577312\n",
      "Processing GSM4577313\n",
      "Processing GSM4577314\n",
      "Processing GSM4577315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "... storing 'sample_names' as categorical\n"
     ]
    }
   ],
   "source": [
    "#list sample and load metadata\n",
    "meta = pd.read_csv(\"./samplemeta.tsv\",sep='\\t')\n",
    "# define path the location of the count matrices for every sample listed in the metadata file loaded\n",
    "folderbase = {\"Luz\" : \"/lustre/scratch117/cellgen/team292/lh20/revision/\", \"Wang\" : \"/lustre/scratch117/cellgen/team205/sharedData/lh20/endometrium-sra-map/\"}\n",
    "folderinner = {\"Luz\" : \"/\", \"Wang\" : \"/counts/Gene/filtered/\"}\n",
    "folderlist = [folderbase[meta[\"StudyName\"][i]] + meta[\"FolderName\"][i] + folderinner[meta[\"StudyName\"][i]] for i in range(meta.shape[0])]\n",
    "adata = lpy.createAnnData(folderlist, meta[\"FolderName\"],doqcplots=True, doinspect=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35;46;1mAdd metadata from sample table to anndata\u001b[0m\u001b[34m\n",
      "def addMetadata(adata, metadata, obs_key, meta_key, doinspect=False):\n",
      "    if doinspect is True: print(\"\\033[35;46;1mAdd metadata from sample table to anndata\\033[0m\\033[34m\"); print(inspect.getsource(addMetadata));print(\"\\033[31;43;1mExecution:\\033[0m\")\n",
      "    aslist = metadata[meta_key].tolist()\n",
      "    rowmap = {i : aslist.index(i)  for i in aslist}\n",
      "    for val in metadata.columns:\n",
      "        if val != meta_key:\n",
      "            aslist = metadata[val].tolist()\n",
      "            adata.obs[val] = [aslist[rowmap[i]] for i in adata.obs[obs_key] ]\n",
      "    return adata;\n",
      "\n",
      "\u001b[31;43;1mExecution:\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#add metadata from data table\n",
    "adata = lpy.addMetadata(adata, meta, \"sample_names\", \"FolderName\", doinspect=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add metadata from other dataset\n",
    "fname_umap = wget.download(\"https://ftp.ncbi.nlm.nih.gov/geo/series/GSE111nnn/GSE111976/suppl/GSE111976_umap_endo_10x.csv.gz\")\n",
    "fname_meta = wget.download(\"https://ftp.ncbi.nlm.nih.gov/geo/series/GSE111nnn/GSE111976/suppl/GSE111976_summary_10x_day_donor_ctype.csv.gz\")\n",
    "map = {\"GSM4577306\": \"14\", \"GSM4577307\":\"19\", \"GSM4577308\" : \"20\", \"GSM4577309\" : \"29\", \"GSM4577310\": \"39\", \"GSM4577311\":\"41\", \"GSM4577312\":\"57\", \"GSM4577313\":\"58\",\"GSM4577314\":\"60\",\"GSM4577315\":\"63\"}\n",
    "wang_umap = pd.read_csv(fname_umap)\n",
    "wang_celltypes = pd.read_csv(fname_meta)\n",
    "wang_name = wang_umap[\"Unnamed: 0\"].tolist()\n",
    "del wang_umap[\"Unnamed: 0\"]\n",
    "wang_umap = np.array(wang_umap)\n",
    "tmap = {}\n",
    "for i in range(len(adata.obs_names)):\n",
    "    if adata.obs[\"sample_names\"][i] in map.keys():\n",
    "        tmap.update( {re.sub(adata.obs[\"sample_names\"][i], map[adata.obs[\"sample_names\"][i]], adata.obs_names[i]) : i})\n",
    "\n",
    "adata.obs[\"Wang_celltype\"] = \"\"\n",
    "adata.obsm[\"X_Wang_umap\"] = np.zeros( (len(adata.obs_names), 2) )\n",
    "for i in range(len(wang_name)):\n",
    "    if wang_name[i] in tmap.keys():\n",
    "        adata.obsm[\"X_Wang_umap\"][tmap[wang_name[i]],:] = wang_umap[i,:]\n",
    "        adata.obs[\"Wang_celltype\"][tmap[wang_name[i]]] = wang_celltypes[\"cell_type\"][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35;46;1mDetect Doublets and defining cells to filter\u001b[0m\u001b[34m\n",
      "def scrub(adata, batch_obsattrib, bonf_threshold = 0.01, add_qc_metrics=False,mito_prefix= \"MT-\", obskey_cellfilter = \"filtered_cells\", add_cell_filter={\"max_percent_mito\": 0.15, \"scrublet_local_pred\": False}, doinspect=False):\n",
      "    if doinspect is True: print(\"\\033[35;46;1mDetect Doublets and defining cells to filter\\033[0m\\033[34m\"); print(inspect.getsource(scrub));print(\"\\033[31;43;1mExecution:\\033[0m\")\n",
      "    \n",
      "    import scrublet as scr\n",
      "    import scanpy as sc\n",
      "    print(\"spliting data using attribute \" + batch_obsattrib)\n",
      "    adatas = splitAnnData(adata, batch_obsattrib)\n",
      "\n",
      "    if (add_qc_metrics):\n",
      "        mito_genes = [name for name in adata.var_names if name.startswith(mito_prefix)]\n",
      "        adata.obs['log2p1_RNA_count'] = np.log1p(adata.X.sum(axis=1).A1) / math.log(2)\n",
      "        adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1\n",
      "\n",
      "    dalist = list(adata.obs_names)\n",
      "    tmap = {}\n",
      "    for i in range(len(adata.obs_names)):\n",
      "        tmap.update( {adata.obs_names[i] : i})\n",
      "    adata.obs[\"scrublet_pred\"] = False\n",
      "    adata.obs[\"scrublet_local_pred\"] = False\n",
      "    adata.obs[\"scrublet_score\"] = 0.0\n",
      "    adata.obs[\"scrublet_cluster_score\"] = 0.0\n",
      "\n",
      "    # Luz's double clustering approach\n",
      "    for i in range(len(adatas)):\n",
      "\n",
      "        print(\"processing \" + str(i) + \"/\" + str(len(adatas)))\n",
      "        curscr = scr.Scrublet(adatas[i].X)\n",
      "        doublet_scores, predicted_doublets = curscr.scrub_doublets()\n",
      "        adatas[i].obs['scrublet_score'] = doublet_scores\n",
      "        adatas[i].obs['scrublet_pred'] = predicted_doublets\n",
      "        #overcluster prep. run turbo basic scanpy pipeline\n",
      "        sc.pp.normalize_per_cell(adatas[i], counts_per_cell_after=1e4)\n",
      "        sc.pp.log1p(adatas[i])\n",
      "        sc.pp.highly_variable_genes(adatas[i], min_mean=0.0125, max_mean=3, min_disp=0.5)\n",
      "        adatas[i] = adatas[i][:, adatas[i].var['highly_variable']]\n",
      "        sc.pp.scale(adatas[i], max_value=10)\n",
      "        sc.tl.pca(adatas[i], svd_solver='arpack')\n",
      "        sc.pp.neighbors(adatas[i])\n",
      "        #overclustering proper - do basic clustering first, then cluster each cluster\n",
      "        sc.tl.leiden(adatas[i])\n",
      "        adatas[i].obs['leiden'] = [str(i) for i in adatas[i].obs['leiden']]\n",
      "        for clus in np.unique(adatas[i].obs['leiden']):\n",
      "            adata_sub = adatas[i][adatas[i].obs['leiden']==clus].copy()\n",
      "            sc.tl.leiden(adata_sub)\n",
      "            adata_sub.obs['leiden'] = [clus+','+i for i in adata_sub.obs['leiden']]\n",
      "            adatas[i].obs.loc[adata_sub.obs_names,'leiden'] = adata_sub.obs['leiden']\n",
      "\n",
      "        #compute the cluster scores - the median of Scrublet scores per overclustered cluster\n",
      "        for clus in np.unique(adatas[i].obs['leiden']):\n",
      "            adatas[i].obs.loc[adatas[i].obs['leiden']==clus, 'scrublet_cluster_score'] = \\\n",
      "                np.median(adatas[i].obs.loc[adatas[i].obs['leiden']==clus, 'scrublet_score'])\n",
      "        #now compute doublet p-values. figure out the median and mad (from above-median values) for the distribution\n",
      "        med = np.median(adatas[i].obs['scrublet_cluster_score'])\n",
      "        mask = adatas[i].obs['scrublet_cluster_score']>med\n",
      "        mad = np.median(adatas[i].obs['scrublet_cluster_score'][mask]-med)\n",
      "        #let's do a one-sided test. the Bertie write-up does not address this but it makes sense\n",
      "        zscores = (adatas[i].obs['scrublet_cluster_score'].values - med) / (1.4826 * mad)\n",
      "        #adatas[i].obs['zscore'] = zscores\n",
      "        pvals = 1-scipy.stats.norm.cdf(zscores)\n",
      "        #adatas[i].obs['bh_pval'] = bh(pvals)\n",
      "        pvals = bonf(pvals)\n",
      "        adatas[i].obs['scrublet_local_pred'] = pvals < bonf_threshold\n",
      "        map = [tmap[s] for s in adatas[i].obs_names]\n",
      "        print(\"annoying values\")\n",
      "        for j in range(len(adatas[i].obs_names)):\n",
      "            adata.obs[\"scrublet_pred\" ][map[j]] = adatas[i].obs[\"scrublet_pred\"][j]\n",
      "            adata.obs[\"scrublet_local_pred\"][map[j]] = adatas[i].obs[\"scrublet_local_pred\"][j]\n",
      "            adata.obs[\"scrublet_cluster_score\"][map[j]] =  adatas[i].obs[\"scrublet_cluster_score\"][j]\n",
      "            adata.obs[\"scrublet_score\"][map[j]] =  pvals[j]\n",
      "\n",
      "    adata.obs[\"scrublet_cluster_score\"] = pd.to_numeric(adata.obs[\"scrublet_cluster_score\"], downcast='float')\n",
      "    adata.obs[\"scrublet_score\"] = pd.to_numeric(adata.obs[\"scrublet_score\"], downcast='float')\n",
      "\n",
      "    if add_cell_filter is not None:\n",
      "        curflt = np.zeros(adata.obs.shape[0], dtype=bool) \n",
      "        for k,v in add_cell_filter.items():\n",
      "            if k == \"max_percent_mito\":\n",
      "                print(str(sum(adata.obs['percent_mito'] > v)) + \" cells filtered by mito threshold\")\n",
      "                curflt |= adata.obs['percent_mito'] > v\n",
      "            else:\n",
      "                print(str(sum(adata.obs[k] != v)) + \" cells filtered by \" + k + \" == \" + str(v) + \" criterion\")\n",
      "                curflt |= (adata.obs[k] != v)\n",
      "        adata.obs[obskey_cellfilter] = curflt\n",
      "    return adata\n",
      "\n",
      "\u001b[31;43;1mExecution:\u001b[0m\n",
      "spliting data using attribute sample_names\n",
      "processing 0/21\n",
      "Preprocessing...\n",
      "Simulating doublets...\n",
      "Embedding transcriptomes using PCA...\n",
      "Calculating doublet scores...\n",
      "Automatically set threshold at doublet score = 0.72\n",
      "Detected doublet rate = 0.3%\n",
      "Estimated detectable doublet fraction = 5.8%\n",
      "Overall doublet rate:\n",
      "\tExpected   = 10.0%\n",
      "\tEstimated  = 4.4%\n",
      "Elapsed time: 13.7 seconds\n",
      "annoying values\n",
      "processing 1/21\n",
      "Preprocessing...\n",
      "Simulating doublets...\n",
      "Embedding transcriptomes using PCA...\n",
      "Calculating doublet scores...\n",
      "Warning: failed to automatically identify doublet score threshold. Run `call_doublets` with user-specified threshold.\n",
      "Elapsed time: 33.6 seconds\n",
      "annoying values\n",
      "processing 2/21\n",
      "Preprocessing...\n",
      "Simulating doublets...\n",
      "Embedding transcriptomes using PCA...\n",
      "Calculating doublet scores...\n",
      "Automatically set threshold at doublet score = 0.22\n",
      "Detected doublet rate = 8.5%\n",
      "Estimated detectable doublet fraction = 74.5%\n",
      "Overall doublet rate:\n",
      "\tExpected   = 10.0%\n",
      "\tEstimated  = 11.4%\n",
      "Elapsed time: 2.6 seconds\n",
      "annoying values\n",
      "processing 3/21\n",
      "Preprocessing...\n",
      "Simulating doublets...\n",
      "Embedding transcriptomes using PCA...\n",
      "Calculating doublet scores...\n",
      "Automatically set threshold at doublet score = 0.58\n",
      "Detected doublet rate = 0.8%\n",
      "Estimated detectable doublet fraction = 18.6%\n",
      "Overall doublet rate:\n",
      "\tExpected   = 10.0%\n",
      "\tEstimated  = 4.4%\n",
      "Elapsed time: 5.3 seconds\n",
      "annoying values\n",
      "processing 4/21\n",
      "Preprocessing...\n",
      "Simulating doublets...\n",
      "Embedding transcriptomes using PCA...\n",
      "Calculating doublet scores...\n",
      "Automatically set threshold at doublet score = 0.51\n",
      "Detected doublet rate = 0.2%\n",
      "Estimated detectable doublet fraction = 20.2%\n",
      "Overall doublet rate:\n",
      "\tExpected   = 10.0%\n",
      "\tEstimated  = 1.0%\n",
      "Elapsed time: 0.4 seconds\n",
      "annoying values\n",
      "processing 5/21\n",
      "Preprocessing...\n",
      "Simulating doublets...\n",
      "Embedding transcriptomes using PCA...\n",
      "Calculating doublet scores...\n",
      "Automatically set threshold at doublet score = 0.60\n",
      "Detected doublet rate = 0.5%\n",
      "Estimated detectable doublet fraction = 16.7%\n",
      "Overall doublet rate:\n",
      "\tExpected   = 10.0%\n",
      "\tEstimated  = 3.0%\n",
      "Elapsed time: 1.8 seconds\n",
      "annoying values\n",
      "processing 6/21\n",
      "Preprocessing...\n",
      "Simulating doublets...\n",
      "Embedding transcriptomes using PCA...\n",
      "Calculating doublet scores...\n",
      "Automatically set threshold at doublet score = 0.62\n",
      "Detected doublet rate = 0.3%\n",
      "Estimated detectable doublet fraction = 16.2%\n",
      "Overall doublet rate:\n",
      "\tExpected   = 10.0%\n",
      "\tEstimated  = 1.6%\n",
      "Elapsed time: 1.6 seconds\n",
      "annoying values\n",
      "processing 7/21\n",
      "Preprocessing...\n",
      "Simulating doublets...\n",
      "Embedding transcriptomes using PCA...\n",
      "Calculating doublet scores...\n",
      "Automatically set threshold at doublet score = 0.38\n",
      "Detected doublet rate = 2.2%\n",
      "Estimated detectable doublet fraction = 51.4%\n",
      "Overall doublet rate:\n",
      "\tExpected   = 10.0%\n",
      "\tEstimated  = 4.2%\n",
      "Elapsed time: 2.0 seconds\n",
      "annoying values\n",
      "processing 8/21\n",
      "Preprocessing...\n",
      "Simulating doublets...\n",
      "Embedding transcriptomes using PCA...\n",
      "Calculating doublet scores...\n",
      "Automatically set threshold at doublet score = 0.39\n",
      "Detected doublet rate = 4.8%\n",
      "Estimated detectable doublet fraction = 47.6%\n",
      "Overall doublet rate:\n",
      "\tExpected   = 10.0%\n",
      "\tEstimated  = 10.1%\n",
      "Elapsed time: 7.6 seconds\n",
      "annoying values\n",
      "processing 9/21\n",
      "Preprocessing...\n",
      "Simulating doublets...\n",
      "Embedding transcriptomes using PCA...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating doublet scores...\n",
      "Automatically set threshold at doublet score = 0.68\n",
      "Detected doublet rate = 0.1%\n",
      "Estimated detectable doublet fraction = 9.7%\n",
      "Overall doublet rate:\n",
      "\tExpected   = 10.0%\n",
      "\tEstimated  = 1.3%\n",
      "Elapsed time: 3.2 seconds\n",
      "annoying values\n",
      "processing 10/21\n",
      "Preprocessing...\n",
      "Simulating doublets...\n",
      "Embedding transcriptomes using PCA...\n",
      "Calculating doublet scores...\n",
      "Automatically set threshold at doublet score = 0.29\n",
      "Detected doublet rate = 10.3%\n",
      "Estimated detectable doublet fraction = 60.3%\n",
      "Overall doublet rate:\n",
      "\tExpected   = 10.0%\n",
      "\tEstimated  = 17.1%\n",
      "Elapsed time: 11.6 seconds\n",
      "annoying values\n",
      "processing 11/21\n",
      "Preprocessing...\n",
      "Simulating doublets...\n",
      "Embedding transcriptomes using PCA...\n",
      "Calculating doublet scores...\n",
      "Automatically set threshold at doublet score = 0.35\n",
      "Detected doublet rate = 2.9%\n",
      "Estimated detectable doublet fraction = 53.6%\n",
      "Overall doublet rate:\n",
      "\tExpected   = 10.0%\n",
      "\tEstimated  = 5.4%\n",
      "Elapsed time: 4.3 seconds\n",
      "annoying values\n",
      "processing 12/21\n",
      "Preprocessing...\n",
      "Simulating doublets...\n",
      "Embedding transcriptomes using PCA...\n",
      "Calculating doublet scores...\n",
      "Automatically set threshold at doublet score = 0.53\n",
      "Detected doublet rate = 1.2%\n",
      "Estimated detectable doublet fraction = 25.3%\n",
      "Overall doublet rate:\n",
      "\tExpected   = 10.0%\n",
      "\tEstimated  = 4.5%\n",
      "Elapsed time: 0.7 seconds\n",
      "annoying values\n",
      "processing 13/21\n",
      "Preprocessing...\n",
      "Simulating doublets...\n",
      "Embedding transcriptomes using PCA...\n",
      "Calculating doublet scores...\n",
      "Automatically set threshold at doublet score = 0.59\n",
      "Detected doublet rate = 1.2%\n",
      "Estimated detectable doublet fraction = 22.8%\n",
      "Overall doublet rate:\n",
      "\tExpected   = 10.0%\n",
      "\tEstimated  = 5.3%\n",
      "Elapsed time: 1.3 seconds\n",
      "annoying values\n",
      "processing 14/21\n",
      "Preprocessing...\n",
      "Simulating doublets...\n",
      "Embedding transcriptomes using PCA...\n",
      "Calculating doublet scores...\n",
      "Automatically set threshold at doublet score = 0.43\n",
      "Detected doublet rate = 2.8%\n",
      "Estimated detectable doublet fraction = 40.9%\n",
      "Overall doublet rate:\n",
      "\tExpected   = 10.0%\n",
      "\tEstimated  = 6.9%\n",
      "Elapsed time: 2.6 seconds\n",
      "annoying values\n",
      "processing 15/21\n",
      "Preprocessing...\n",
      "Simulating doublets...\n",
      "Embedding transcriptomes using PCA...\n",
      "Calculating doublet scores...\n",
      "Automatically set threshold at doublet score = 0.73\n",
      "Detected doublet rate = 0.2%\n",
      "Estimated detectable doublet fraction = 8.9%\n",
      "Overall doublet rate:\n",
      "\tExpected   = 10.0%\n",
      "\tEstimated  = 2.6%\n",
      "Elapsed time: 5.6 seconds\n",
      "annoying values\n",
      "processing 16/21\n",
      "Preprocessing...\n",
      "Simulating doublets...\n",
      "Embedding transcriptomes using PCA...\n",
      "Calculating doublet scores...\n",
      "Automatically set threshold at doublet score = 0.51\n",
      "Detected doublet rate = 1.8%\n",
      "Estimated detectable doublet fraction = 37.4%\n",
      "Overall doublet rate:\n",
      "\tExpected   = 10.0%\n",
      "\tEstimated  = 4.9%\n",
      "Elapsed time: 3.2 seconds\n",
      "annoying values\n",
      "processing 17/21\n",
      "Preprocessing...\n",
      "Simulating doublets...\n",
      "Embedding transcriptomes using PCA...\n",
      "Calculating doublet scores...\n",
      "Automatically set threshold at doublet score = 0.41\n",
      "Detected doublet rate = 3.4%\n",
      "Estimated detectable doublet fraction = 54.1%\n",
      "Overall doublet rate:\n",
      "\tExpected   = 10.0%\n",
      "\tEstimated  = 6.3%\n",
      "Elapsed time: 7.7 seconds\n",
      "annoying values\n",
      "processing 18/21\n",
      "Preprocessing...\n",
      "Simulating doublets...\n",
      "Embedding transcriptomes using PCA...\n",
      "Calculating doublet scores...\n",
      "Automatically set threshold at doublet score = 0.33\n",
      "Detected doublet rate = 4.2%\n",
      "Estimated detectable doublet fraction = 61.5%\n",
      "Overall doublet rate:\n",
      "\tExpected   = 10.0%\n",
      "\tEstimated  = 6.8%\n",
      "Elapsed time: 3.1 seconds\n",
      "annoying values\n",
      "processing 19/21\n",
      "Preprocessing...\n",
      "Simulating doublets...\n",
      "Embedding transcriptomes using PCA...\n",
      "Calculating doublet scores...\n",
      "Automatically set threshold at doublet score = 0.41\n",
      "Detected doublet rate = 2.0%\n",
      "Estimated detectable doublet fraction = 51.8%\n",
      "Overall doublet rate:\n",
      "\tExpected   = 10.0%\n",
      "\tEstimated  = 3.9%\n",
      "Elapsed time: 3.5 seconds\n",
      "annoying values\n",
      "processing 20/21\n",
      "Preprocessing...\n",
      "Simulating doublets...\n",
      "Embedding transcriptomes using PCA...\n",
      "Calculating doublet scores...\n",
      "Automatically set threshold at doublet score = 0.30\n",
      "Detected doublet rate = 5.1%\n",
      "Estimated detectable doublet fraction = 68.8%\n",
      "Overall doublet rate:\n",
      "\tExpected   = 10.0%\n",
      "\tEstimated  = 7.4%\n",
      "Elapsed time: 5.7 seconds\n",
      "annoying values\n",
      "17234 cells filtered by mito threshold\n",
      "3515 cells filtered by scrublet_local_pred == False criterion\n"
     ]
    }
   ],
   "source": [
    "#find doublets and identidy cells with high mitochondrial content, and labels such cells as \"fitlered cells\"\n",
    "obskey_filteredcells = \"filtered_cells\"\n",
    "adata = lpy.scrub(adata, \"sample_names\",obskey_cellfilter= obskey_filteredcells,add_cell_filter={\"max_percent_mito\": 0.15, \"scrublet_local_pred\": False}, doinspect=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35;46;1mPrepare Data for Scvi/TotalVi\u001b[0m\u001b[34m\n",
      "def scvi_prepare(anndatapath, field, cellfilter = None, nbgenes = 5000, genes_to_filter= None, use_ccfilter_prefix=None, citeseqkey = \"protein_expression\", use_raw_slot_instead =None, min_cell_threshold= 0, doinspect=False):\n",
      "    if doinspect is True: print(\"\\033[35;46;1mPrepare Data for Scvi/TotalVi\\033[0m\\033[34m\"); print(inspect.getsource(scvi_prepare));print(\"\\033[31;43;1mExecution:\\033[0m\")\n",
      "    if use_ccfilter_prefix is not None :\n",
      "        genes_to_filter = [use_ccfilter_prefix + x for x in [\"HMGB2\",\"CDK1\",\"NUSAP1\",\"UBE2C\",\"BIRC5\",\"TPX2\",\"TOP2A\",\"NDC80\",\"CKS2\",\"NUF2\",\"CKS1B\",\"MKI67\",\"TMPO\",\"CENPF\",\"TACC3\",\"FAM64A\",\"SMC4\",\"CCNB2\",\"CKAP2L\",\"CKAP2\",\"AURKB\",\"BUB1\",\"KIF11\",\"ANP32E\",\"TUBB4B\",\"GTSE1\",\"KIF20B\",\"HJURP\",\"CDCA3\",\"HN1\",\"CDC20\",\"TTK\",\"CDC25C\",\"KIF2C\",\"RANGAP1\",\"NCAPD2\",\"DLGAP5\",\"CDCA2\",\"CDCA8\",\"ECT2\",\"KIF23\",\"HMMR\",\"AURKA\",\"PSRC1\",\"ANLN\",\"LBR\",\"CKAP5\",\"CENPE\",\"CTCF\",\"NEK2\",\"G2E3\",\"GAS2L3\",\"CBX5\",\"CENPA\",\"MCM5\",\"PCNA\",\"TYMS\",\"FEN1\",\"MCM2\",\"MCM4\",\"RRM1\",\"UNG\",\"GINS2\",\"MCM6\",\"CDCA7\",\"DTL\",\"PRIM1\",\"UHRF1\",\"MLF1IP\",\"HELLS\",\"RFC2\",\"RPA2\",\"NASP\",\"RAD51AP1\",\"GMNN\",\"WDR76\",\"SLBP\",\"CCNE2\",\"UBR7\",\"POLD3\",\"MSH2\",\"ATAD2\",\"RAD51\",\"RRM2\",\"CDC45\",\"CDC6\",\"EXO1\",\"TIPIN\",\"DSCC1\",\"BLM\",\"CASP8AP2\",\"USP1\",\"CLSPN\",\"POLA1\",\"CHAF1B\",\"BRIP1\",\"E2F8\"]]\n",
      "    if (isinstance(anndatapath, str)): cite = anndata.read_h5ad(anndatapath)\n",
      "    else: cite = anndatapath\n",
      "    if use_raw_slot_instead is None: # default behavior, check for the existance of the raw slot\n",
      "        use_raw_slot_instead = cite.raw is not None\n",
      "    \n",
      "    split = splitAnnData(cite, field, entryfilter= cellfilter, getnames = True, use_raw_slot_instead= use_raw_slot_instead,min_cell_threshold=min_cell_threshold)\n",
      "    if genes_to_filter is not None:\n",
      "        for i in range(len(split[\"datalist\"])):\n",
      "            split[\"datalist\"][i] = split[\"datalist\"][i][:, [g not in genes_to_filter for g in split[\"datalist\"][i].var_names]]\n",
      "    \n",
      "    \n",
      "\n",
      "    dataset = GeneExpressionDataset()\n",
      "    if citeseqkey in cite.obsm.keys():\n",
      "        if citeseqkey is not \"protein_expression\":\n",
      "            for i in range(len(split[\"datalist\"])):\n",
      "                split[\"datalist\"][i].obsm[\"protein_expression\"] = split[\"datalist\"][i].obsm[citeseqkey]\n",
      "        dataset.populate_from_datasets([AnnDatasetFromAnnData(ad=s,cell_measurements_col_mappings={\"protein_expression\":\"protein_names\"})   for s in split[\"datalist\"] ])\n",
      "    else:\n",
      "        dataset.populate_from_datasets([AnnDatasetFromAnnData(ad=s)   for s in split[\"datalist\"] ])\n",
      "\n",
      "    if nbgenes != 0:\n",
      "        dataset.subsample_genes(nbgenes)\n",
      "    return{\"dataset\":dataset , \"names\" : split[\"ordered\"]}\n",
      "\n",
      "\u001b[31;43;1mExecution:\u001b[0m\n",
      "[2020-10-30 14:51:58,252] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-10-30 14:51:58,826] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:51:58,828] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:52:00,051] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:52:00,258] INFO - scvi.dataset.dataset | Downsampled from 4310 to 4310 cells\n",
      "[2020-10-30 14:52:00,793] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:52:00,796] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:52:01,503] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:52:01,627] INFO - scvi.dataset.dataset | Downsampled from 20816 to 20816 cells\n",
      "[2020-10-30 14:52:01,669] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-10-30 14:52:02,238] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:52:02,240] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:52:03,352] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:52:03,558] INFO - scvi.dataset.dataset | Downsampled from 4335 to 4335 cells\n",
      "[2020-10-30 14:52:03,664] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-10-30 14:52:04,457] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:52:04,459] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:52:06,141] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:52:06,431] INFO - scvi.dataset.dataset | Downsampled from 6072 to 6072 cells\n",
      "[2020-10-30 14:52:06,462] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-10-30 14:52:06,560] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:52:06,562] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:52:06,713] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:52:06,742] INFO - scvi.dataset.dataset | Downsampled from 603 to 603 cells\n",
      "[2020-10-30 14:52:06,784] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-10-30 14:52:07,057] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:52:07,060] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:52:07,646] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:52:07,739] INFO - scvi.dataset.dataset | Downsampled from 1958 to 1958 cells\n",
      "[2020-10-30 14:52:07,786] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-10-30 14:52:08,052] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:52:08,055] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:52:08,639] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:52:08,728] INFO - scvi.dataset.dataset | Downsampled from 1870 to 1870 cells\n",
      "[2020-10-30 14:52:08,775] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-10-30 14:52:09,057] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:52:09,060] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:52:09,680] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:52:09,776] INFO - scvi.dataset.dataset | Downsampled from 2025 to 2025 cells\n",
      "[2020-10-30 14:52:09,904] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-10-30 14:52:10,862] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:52:10,864] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:52:12,847] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:52:13,168] INFO - scvi.dataset.dataset | Downsampled from 6737 to 6737 cells\n",
      "[2020-10-30 14:52:13,263] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-10-30 14:52:13,668] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:52:13,671] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:52:14,589] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:52:14,729] INFO - scvi.dataset.dataset | Downsampled from 2945 to 2945 cells\n",
      "[2020-10-30 14:52:14,913] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:52:14,916] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:52:15,086] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:52:15,123] INFO - scvi.dataset.dataset | Downsampled from 12665 to 12665 cells\n",
      "[2020-10-30 14:52:15,182] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-10-30 14:52:15,516] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:52:15,519] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:52:16,265] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:52:16,380] INFO - scvi.dataset.dataset | Downsampled from 2415 to 2415 cells\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-10-30 14:52:16,408] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-10-30 14:52:16,542] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:52:16,544] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:52:16,801] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:52:16,844] INFO - scvi.dataset.dataset | Downsampled from 879 to 879 cells\n",
      "[2020-10-30 14:52:16,880] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-10-30 14:52:17,081] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:52:17,084] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:52:17,510] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:52:17,577] INFO - scvi.dataset.dataset | Downsampled from 1408 to 1408 cells\n",
      "[2020-10-30 14:52:17,644] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-10-30 14:52:18,048] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:52:18,050] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:52:18,953] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:52:19,096] INFO - scvi.dataset.dataset | Downsampled from 3002 to 3002 cells\n",
      "[2020-10-30 14:52:19,198] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-10-30 14:52:19,978] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:52:19,981] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:52:21,663] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:52:21,949] INFO - scvi.dataset.dataset | Downsampled from 6007 to 6007 cells\n",
      "[2020-10-30 14:52:22,007] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-10-30 14:52:22,327] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:52:22,329] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:52:22,984] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:52:23,096] INFO - scvi.dataset.dataset | Downsampled from 2368 to 2368 cells\n",
      "[2020-10-30 14:52:23,218] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-10-30 14:52:24,134] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:52:24,137] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:52:26,182] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:52:26,522] INFO - scvi.dataset.dataset | Downsampled from 7095 to 7095 cells\n",
      "[2020-10-30 14:52:26,608] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-10-30 14:52:27,064] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:52:27,067] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:52:28,034] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:52:28,195] INFO - scvi.dataset.dataset | Downsampled from 3384 to 3384 cells\n",
      "[2020-10-30 14:52:28,270] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-10-30 14:52:28,684] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:52:28,687] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:52:29,589] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:52:29,735] INFO - scvi.dataset.dataset | Downsampled from 3069 to 3069 cells\n",
      "[2020-10-30 14:52:29,819] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-10-30 14:52:30,647] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:52:30,649] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:52:32,332] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:52:32,635] INFO - scvi.dataset.dataset | Downsampled from 6344 to 6344 cells\n",
      "[2020-10-30 14:52:32,661] INFO - scvi.dataset.dataset | Merging datasets. Input objects are modified in place.\n",
      "[2020-10-30 14:52:32,663] INFO - scvi.dataset.dataset | Gene names and cell measurement names are assumed to have a non-null intersection between datasets.\n",
      "[2020-10-30 14:52:33,097] INFO - scvi.dataset.dataset | Keeping 28518 genes\n",
      "[2020-10-30 14:52:34,376] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:52:34,922] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:52:34,924] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:52:35,727] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:52:35,858] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:52:35,861] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:52:37,074] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:52:37,645] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:52:37,648] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:52:39,418] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:52:40,186] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:52:40,189] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:52:40,378] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:52:40,458] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:52:40,461] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:52:41,088] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:52:41,344] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:52:41,346] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:52:41,952] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:52:42,189] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:52:42,191] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:52:42,844] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:52:43,096] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:52:43,098] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:52:45,152] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:52:45,978] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:52:45,981] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:52:46,914] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:52:47,277] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:52:47,280] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:52:47,499] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:52:47,541] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:52:47,543] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:52:48,346] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:52:48,645] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:52:48,647] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:52:48,936] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:52:49,049] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:52:49,051] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-10-30 14:52:49,510] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:52:49,733] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:52:49,735] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:52:50,674] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:52:51,060] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:52:51,063] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:52:52,811] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:52:53,542] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:52:53,544] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:52:54,246] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:52:54,599] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:52:54,601] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:52:56,715] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:52:57,598] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:52:57,600] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:52:58,633] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:52:59,060] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:52:59,063] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:53:00,009] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:53:00,426] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:53:00,429] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:53:02,240] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:53:03,040] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:53:03,042] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:53:38,954] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-10-30 14:53:38,961] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-10-30 14:53:39,780] INFO - scvi.dataset.dataset | extracting highly variable genes using seurat_v3 flavor\n",
      "[2020-10-30 14:53:50,077] INFO - scvi.dataset.dataset | Downsampling from 28518 to 5000 genes\n",
      "[2020-10-30 14:53:52,699] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:53:52,910] INFO - scvi.dataset.dataset | Filtering non-expressing cells.\n",
      "[2020-10-30 14:53:54,377] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-10-30 14:53:54,575] INFO - scvi.dataset.dataset | Downsampled from 100307 to 100307 cells\n"
     ]
    }
   ],
   "source": [
    "# prepare data for scvi, subset out doublets, extrude cc genes (listed in function) and populate scvi objects \n",
    "sp = lpy.scvi_prepare(adata, \"sample_names\", adata.obs[obskey_filteredcells] == False, use_ccfilter_prefix=\"\", doinspect=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35;46;1mRun scvi\u001b[0m\u001b[34m\n",
      "def runSCVI(dataset, nbstep = 500, n_latent = 64, doinspect= False):\n",
      "    if doinspect is True: print(\"\\033[35;46;1mRun scvi\\033[0m\\033[34m\"); print(inspect.getsource(runSCVI));print(\"\\033[31;43;1mExecution:\\033[0m\")\n",
      "    vae = VAE(dataset.nb_genes, n_batch= dataset.n_batches, n_labels= dataset.n_labels, n_latent = n_latent)\n",
      "    trainer = UnsupervisedTrainer(vae, dataset, train_size=0.9, frequency=5, use_cuda=True)\n",
      "    trainer.train(n_epochs=nbstep)\n",
      "    full = trainer.create_posterior(trainer.model, dataset, indices=np.arange(len(dataset)))\n",
      "    return(full.sequential().get_latent()[0])\n",
      "\n",
      "\u001b[31;43;1mExecution:\u001b[0m\n",
      "[2020-10-30 14:54:07,272] INFO - scvi.inference.inference | KL warmup for 400 epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f7422f8f0ea4577971fb058dcb85e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training', max=500.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# run scvi\n",
    "latent = lpy.runSCVI(sp[\"dataset\"], doinspect=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35;46;1mCompute Clusters and Reduces representations\u001b[0m\u001b[34m\n",
      "def insertLatent(adata, latent , latent_key= \"latent\", umap_key= \"X_umap\", tsne_key = \"X_tsne\", leiden_key = \"leiden\", rename_cluster_key= None,cellfilter = None, cellnames =None, leiden_resolution=1.0,doinspect=False):\n",
      "    if doinspect is True: print(\"\\033[35;46;1mCompute Clusters and Reduces representations\\033[0m\\033[34m\"); print(inspect.getsource(insertLatent));print(\"\\033[31;43;1mExecution:\\033[0m\")\n",
      "\n",
      "\n",
      "    if cellnames is None:\n",
      "        #order of full must match\n",
      "        assert latent.shape[0] == len(adata.obs_names),  \"cell names need for be provided if size of latent mismatches adata\"\n",
      "        map = range(len(adata.obs_names))\n",
      "        if latent_key is not None:\n",
      "            adata.obsm[latent_key] = latent\n",
      "    else:\n",
      "        print(\"defining permutation\")\n",
      "        dalist = list(adata.obs_names)\n",
      "        tmap = {}\n",
      "        for i in range(len(adata.obs_names)):\n",
      "            tmap.update( {adata.obs_names[i] : i})\n",
      "# for i in range(len(adata.obs_names))}\n",
      "        map = [tmap[s] for s in cellnames]\n",
      "        if latent_key is not None:\n",
      "            print(\"Inserting Latent coords\")\n",
      "            adata.obsm[latent_key] = np.zeros( (len(adata.obs_names), latent.shape[1]) )\n",
      "            for i in range(len(map)):\n",
      "                adata.obsm[latent_key][map[i],:] = latent[i,:]\n",
      "    \n",
      "    if umap_key is not None:\n",
      "        import umap.umap_ as umap\n",
      "        print(\"computing UMAP\")\n",
      "        tumap = umap.UMAP(spread=2).fit_transform(latent)\n",
      "        adata.obsm[umap_key] = np.zeros( ( len(adata.obs_names), 2 ) )\n",
      "\n",
      "        print(\"Inserting Umap coords\")\n",
      "        for i in range(len(map)):\n",
      "            adata.obsm[umap_key][map[i],:] = tumap[i,:]\n",
      "\n",
      "    if tsne_key is not None:\n",
      "        import scanpy as sc\n",
      "        print(\"computing Tsne\")\n",
      "        adata_latent = sc.AnnData(latent)\n",
      "\n",
      "        sc.tl.tsne(adata_latent, use_rep='X', n_jobs=8)\n",
      "        adata.obsm[tsne_key] = np.zeros( ( len(adata.obs_names), 2 ) )\n",
      "\n",
      "        print(\"Inserting Tsne coords\")\n",
      "        for i in range(len(map)):\n",
      "            adata.obsm[tsne_key][map[i],:] = adata_latent.obsm[\"X_tsne\"][i,:]\n",
      "\n",
      "    if leiden_key is not None:\n",
      "        import scanpy as sc\n",
      "        adata_latent = sc.AnnData(latent)\n",
      "        print(\"Finding clusters\")\n",
      "        sc.pp.neighbors(adata_latent, use_rep='X', n_neighbors=30, metric='minkowski')\n",
      "        sc.tl.leiden(adata_latent, resolution=leiden_resolution)\n",
      "        adata.obs[leiden_key] = \"filtered\"\n",
      "        #if rename_cluster_key is not None:\n",
      "        #    print(\"Renaming to do...\")\n",
      "        #    for i\n",
      "        #else\n",
      "        #    ctnames = range(200)\n",
      "        \n",
      "        print(\"Inserting Cluster Id\")\n",
      "        for i in range(len(map)):\n",
      "            adata.obs[leiden_key][map[i]] = adata_latent.obs[\"leiden\"][i]\n",
      "    return adata\n",
      "\n",
      "\u001b[31;43;1mExecution:\u001b[0m\n",
      "defining permutation\n",
      "Inserting Latent coords\n",
      "computing UMAP\n",
      "Inserting Umap coords\n",
      "Finding clusters\n",
      "Inserting Cluster Id\n"
     ]
    }
   ],
   "source": [
    "# store latent variable back in th object, and produce UMAP coordinates and leiden clusters \n",
    "adata = lpy.insertLatent(adata,latent, \"scvi_sampl_cc\", \"X_umap_scvi_sampl_cc\", None, \"leiden_scvi_sampl_cc\", cellnames = sp[\"names\"], doinspect=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35;46;1mCompute Clusters and Reduces representations\u001b[0m\u001b[34m\n",
      "def insertLatent(adata, latent , latent_key= \"latent\", umap_key= \"X_umap\", tsne_key = \"X_tsne\", leiden_key = \"leiden\", rename_cluster_key= None,cellfilter = None, cellnames =None, leiden_resolution=1.0,doinspect=False):\n",
      "    if doinspect is True: print(\"\\033[35;46;1mCompute Clusters and Reduces representations\\033[0m\\033[34m\"); print(inspect.getsource(insertLatent));print(\"\\033[31;43;1mExecution:\\033[0m\")\n",
      "\n",
      "\n",
      "    if cellnames is None:\n",
      "        #order of full must match\n",
      "        assert latent.shape[0] == len(adata.obs_names),  \"cell names need for be provided if size of latent mismatches adata\"\n",
      "        map = range(len(adata.obs_names))\n",
      "        if latent_key is not None:\n",
      "            adata.obsm[latent_key] = latent\n",
      "    else:\n",
      "        print(\"defining permutation\")\n",
      "        dalist = list(adata.obs_names)\n",
      "        tmap = {}\n",
      "        for i in range(len(adata.obs_names)):\n",
      "            tmap.update( {adata.obs_names[i] : i})\n",
      "# for i in range(len(adata.obs_names))}\n",
      "        map = [tmap[s] for s in cellnames]\n",
      "        if latent_key is not None:\n",
      "            print(\"Inserting Latent coords\")\n",
      "            adata.obsm[latent_key] = np.zeros( (len(adata.obs_names), latent.shape[1]) )\n",
      "            for i in range(len(map)):\n",
      "                adata.obsm[latent_key][map[i],:] = latent[i,:]\n",
      "    \n",
      "    if umap_key is not None:\n",
      "        import umap.umap_ as umap\n",
      "        print(\"computing UMAP\")\n",
      "        tumap = umap.UMAP(spread=2).fit_transform(latent)\n",
      "        adata.obsm[umap_key] = np.zeros( ( len(adata.obs_names), 2 ) )\n",
      "\n",
      "        print(\"Inserting Umap coords\")\n",
      "        for i in range(len(map)):\n",
      "            adata.obsm[umap_key][map[i],:] = tumap[i,:]\n",
      "\n",
      "    if tsne_key is not None:\n",
      "        import scanpy as sc\n",
      "        print(\"computing Tsne\")\n",
      "        adata_latent = sc.AnnData(latent)\n",
      "\n",
      "        sc.tl.tsne(adata_latent, use_rep='X', n_jobs=8)\n",
      "        adata.obsm[tsne_key] = np.zeros( ( len(adata.obs_names), 2 ) )\n",
      "\n",
      "        print(\"Inserting Tsne coords\")\n",
      "        for i in range(len(map)):\n",
      "            adata.obsm[tsne_key][map[i],:] = adata_latent.obsm[\"X_tsne\"][i,:]\n",
      "\n",
      "    if leiden_key is not None:\n",
      "        import scanpy as sc\n",
      "        adata_latent = sc.AnnData(latent)\n",
      "        print(\"Finding clusters\")\n",
      "        sc.pp.neighbors(adata_latent, use_rep='X', n_neighbors=30, metric='minkowski')\n",
      "        sc.tl.leiden(adata_latent, resolution=leiden_resolution)\n",
      "        adata.obs[leiden_key] = \"filtered\"\n",
      "        #if rename_cluster_key is not None:\n",
      "        #    print(\"Renaming to do...\")\n",
      "        #    for i\n",
      "        #else\n",
      "        #    ctnames = range(200)\n",
      "        \n",
      "        print(\"Inserting Cluster Id\")\n",
      "        for i in range(len(map)):\n",
      "            adata.obs[leiden_key][map[i]] = adata_latent.obs[\"leiden\"][i]\n",
      "    return adata\n",
      "\n",
      "\u001b[31;43;1mExecution:\u001b[0m\n",
      "defining permutation\n",
      "Finding clusters\n",
      "Inserting Cluster Id\n"
     ]
    }
   ],
   "source": [
    "# additionnally produce leiden clusters with resolution 2\n",
    "adata = lpy.insertLatent(adata,latent, None, None, None, \"leidenres2_scvi_sampl_cc\", cellnames = sp[\"names\"],leiden_resolution=2, doinspect=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "... storing 'StudyName' as categorical\n",
      "... storing 'SampleID' as categorical\n",
      "... storing 'DonorID' as categorical\n",
      "... storing 'BiopsyType' as categorical\n",
      "... storing 'Location' as categorical\n",
      "... storing 'Binary Stage' as categorical\n",
      "... storing 'Stage' as categorical\n",
      "... storing 'Day' as categorical\n",
      "... storing 'Women age' as categorical\n",
      "... storing '10x kit' as categorical\n",
      "... storing 'Treatment' as categorical\n",
      "... storing 'Wang_celltype' as categorical\n",
      "... storing 'leiden_scvi_sampl_cc' as categorical\n",
      "... storing 'leidenres2_scvi_sampl_cc' as categorical\n"
     ]
    }
   ],
   "source": [
    "# save object\n",
    "adata.write_h5ad(\"N1-integrated_donors.h5ad\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
