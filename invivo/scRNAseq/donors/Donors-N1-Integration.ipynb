{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration of biopsies and organ donors from 2 studies\n",
    "import anndata, numpy as np, pandas as pd, lpy, scanpy as sc, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35;46;1mCreate AnnData Object\u001b[0m\u001b[34m\n",
      "def createAnnData(folderlist, prefix, souporcell_folderlist = None, souporcell_genodico = None, autoinclude=[\"percent_mito\", \"log2p1_count\", \"n_genes\"], min_cell_per_gene_allowed=3, min_gene_per_cell_allowed=500, sample_obskey = \"sample_names\",doqcplots=False, doinspect=False, mitogeneprefix=\"MT-\", do_log2_normalize=True):\n",
      "    if doinspect is True: print(\"\\033[35;46;1mCreate AnnData Object\\033[0m\\033[34m\"); print(inspect.getsource(createAnnData));print(\"\\033[31;43;1mExecution:\\033[0m\")\n",
      "    adatas = []\n",
      "    def pickname(x,y):\n",
      "        if (y == \"singlet\"): return(\"_genotype_\" +str(x))\n",
      "        else: return(\"_doublet\")\n",
      "    def pickname2(x,y,z):\n",
      "        if (y == \"singlet\"): return(z[int(x)])\n",
      "        else: return(\"doublet\")\n",
      "    for i in range(len(folderlist)):\n",
      "        print(\"Processing \" + prefix[i])\n",
      "        adatas.append(sc.read_10x_mtx(folderlist[i]))\n",
      "        if souporcell_folderlist is not None:\n",
      "            try:\n",
      "                res = pd.read_csv(souporcell_folderlist[i] + \"clusters.tsv\",sep='\\t')\n",
      "                if (res.shape[0] != len(adatas[i].obs_names)): raise ValueError( prefix[i] + \" data and soupx barcodes mismatches!\")\n",
      "                adatas[i].obs[\"souporcell\"] = res.status.to_list()\n",
      "                if (souporcell_genodico is None) or (prefix[i] not in souporcell_genodico.keys()):\n",
      "                    adatas[i].obs[\"demultiplexed\"] = [prefix[i] + pickname(x,y)   for x,y in zip( res.assignment.to_list() , res.status.to_list()) ]\n",
      "                else:\n",
      "                    adatas[i].obs[\"demultiplexed\"] = [pickname2(x,y, souporcell_genodico[prefix[i]] )   for x,y in zip( res.assignment.to_list() , res.status.to_list()) ]\n",
      "            except:\n",
      "                print(\"No valid genotyping data for \"+ str(len(adatas[i].obs_names)) + \" cells within \" + prefix[i] + \"! Setting everything to singlets\")\n",
      "                adatas[i].obs[\"souporcell\"] = \"singlet\"\n",
      "                if (souporcell_genodico is None):\n",
      "                    adatas[i].obs[\"demultiplexed\"] = prefix[i] + \"_genotype_0\"\n",
      "                else:\n",
      "                    adatas[i].obs[\"demultiplexed\"] = souporcell_genodico[prefix[i]][0]\n",
      "                    \n",
      "        adatas[i].obs[sample_obskey] = [prefix[i] for hehe in adatas[i].obs_names]\n",
      "        adatas[i].obs_names = [prefix[i] + \"_\" + x for x in adatas[i].obs_names]\n",
      "        if \"n_count\" in autoinclude:\n",
      "            adatas[i].obs['n_count'] = np.sum(adatas[i].X, axis=1).A1\n",
      "        if \"log2p1_count\" in autoinclude:\n",
      "            adatas[i].obs['log2p1_count'] = np.log1p(np.sum(adatas[i].X, axis=1).A1) / math.log(2)\n",
      "        if \"percent_mito\" in autoinclude:\n",
      "            mito_genes = [name for name in adatas[i].var_names if name.startswith(mitogeneprefix)]\n",
      "            adatas[i].obs['percent_mito'] = np.sum(adatas[i][:, mito_genes].X, axis=1).A1 / np.sum(adatas[i].X, axis=1).A1\n",
      "        if \"n_genes\" in autoinclude:\n",
      "            adatas[i].obs['n_genes'] = np.sum(adatas[i].X != 0, axis=1).A1\n",
      "        adatas[i].obs[sample_obskey].value_counts()\n",
      "    adata = mergeAnnData(adatas,index_unique = None)\n",
      "    if min_gene_per_cell_allowed is not None: sc.pp.filter_cells(adata, min_genes=min_gene_per_cell_allowed)\n",
      "    if min_cell_per_gene_allowed is not None: sc.pp.filter_genes(adata, min_cells=min_cell_per_gene_allowed)\n",
      "    \n",
      "    if doqcplots is True:\n",
      "        if \"log2p1_count\" in autoinclude:\n",
      "            sc.pl.violin(adata, ['log2p1_count'], jitter=0.4, groupby=sample_obskey, rotation=90)\n",
      "        if \"n_genes\" in autoinclude:\n",
      "            sc.pl.violin(adata, ['n_genes'], jitter=0.4, groupby=sample_obskey, rotation=90)\n",
      "        if \"percent_mito\" in autoinclude:\n",
      "            sc.pl.violin(adata, ['percent_mito'], jitter=0.4, groupby=sample_obskey, rotation=90)\n",
      "    if do_log2_normalize:\n",
      "        adata.raw = adata.copy()\n",
      "        adata.X = np.log1p(adata.X) / math.log(2)\n",
      "    if \"log2p1_count\" in autoinclude:\n",
      "        adata.obs[\"log2p1_count\"] = pd.to_numeric(adata.obs[\"log2p1_count\"], downcast='float')\n",
      "    if \"percent_mito\" in autoinclude:\n",
      "        adata.obs[\"percent_mito\"] = pd.to_numeric(adata.obs[\"percent_mito\"], downcast='float')\n",
      "    return adata;\n",
      "\n",
      "\u001b[31;43;1mExecution:\u001b[0m\n",
      "Processing 4861STDY7309368\n",
      "Processing 4861STDY7309369\n",
      "Processing 4861STDY7309370\n",
      "Processing 4861STDY7309371\n",
      "Processing 4861STDY7387181\n",
      "Processing 4861STDY7387182\n",
      "Processing 4861STDY7387183\n",
      "Processing 4861STDY7462245\n",
      "Processing 4861STDY7462246\n",
      "Processing 4861STDY7462247\n",
      "Processing 4861STDY7462248\n",
      "Processing 4861STDY7771115\n",
      "Processing 4861STDY7771123\n",
      "Processing MRC_Endo8625698\n",
      "Processing MRC_Endo8625699\n",
      "Processing MRC_Endo8712024\n",
      "Processing MRC_Endo8712032\n",
      "Processing MRC_Endo8715415\n",
      "Processing MRC_Endo8715416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "... storing 'sample_names' as categorical\n",
      "... storing 'feature_types-0' as categorical\n",
      "... storing 'feature_types-1' as categorical\n",
      "... storing 'feature_types-2' as categorical\n",
      "... storing 'feature_types-3' as categorical\n"
     ]
    }
   ],
   "source": [
    "meta = pd.read_csv(\"./samplemeta.tsv\",sep='\\t')\n",
    "# define path the location of the count matrices for every sample listed in the metadata file loaded\n",
    "folderbase = \"/lustre/scratch117/cellgen/team292/lh20/revision/\"\n",
    "folderlist = [folderbase + meta[\"FolderName\"][i] + '/' for i in range(meta.shape[0])]\n",
    "adata = lpy.createAnnData(folderlist, meta[\"FolderName\"],doqcplots=True, doinspect=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35;46;1mAdd metadata from sample table to anndata\u001b[0m\u001b[34m\n",
      "def addMetadata(adata, metadata, obs_key, meta_key, doinspect=False):\n",
      "    if doinspect is True: print(\"\\033[35;46;1mAdd metadata from sample table to anndata\\033[0m\\033[34m\"); print(inspect.getsource(addMetadata));print(\"\\033[31;43;1mExecution:\\033[0m\")\n",
      "    aslist = metadata[meta_key].tolist()\n",
      "    rowmap = {i : aslist.index(i)  for i in aslist}\n",
      "    for val in metadata.columns:\n",
      "        if val != meta_key:\n",
      "            aslist = metadata[val].tolist()\n",
      "            adata.obs[val] = [aslist[rowmap[i]] for i in adata.obs[obs_key] ]\n",
      "    return adata;\n",
      "\n",
      "\u001b[31;43;1mExecution:\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#add metadata from data table\n",
    "adata = lpy.addMetadata(adata, meta, \"sample_names\", \"FolderName\", doinspect=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35;46;1mDetect Doublets and defining cells to filter\u001b[0m\u001b[34m\n",
      "def scrub(adata, batch_obsattrib, bonf_threshold = 0.01, add_qc_metrics=False,mito_prefix= \"MT-\", obskey_cellfilter = \"filtered_cells\", add_cell_filter={\"max_percent_mito\": 0.15, \"scrublet_local_pred\": False}, doinspect=False):\n",
      "    if doinspect is True: print(\"\\033[35;46;1mDetect Doublets and defining cells to filter\\033[0m\\033[34m\"); print(inspect.getsource(scrub));print(\"\\033[31;43;1mExecution:\\033[0m\")\n",
      "    \n",
      "    import scrublet as scr\n",
      "    import scanpy as sc\n",
      "    print(\"spliting data using attribute \" + batch_obsattrib)\n",
      "    adatas = splitAnnData(adata, batch_obsattrib)\n",
      "\n",
      "    if (add_qc_metrics):\n",
      "        mito_genes = [name for name in adata.var_names if name.startswith(mito_prefix)]\n",
      "        adata.obs['log2p1_RNA_count'] = np.log1p(adata.X.sum(axis=1).A1) / math.log(2)\n",
      "        adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1\n",
      "\n",
      "    dalist = list(adata.obs_names)\n",
      "    tmap = {}\n",
      "    for i in range(len(adata.obs_names)):\n",
      "        tmap.update( {adata.obs_names[i] : i})\n",
      "    adata.obs[\"scrublet_pred\"] = False\n",
      "    adata.obs[\"scrublet_local_pred\"] = False\n",
      "    adata.obs[\"scrublet_score\"] = 0.0\n",
      "    adata.obs[\"scrublet_cluster_score\"] = 0.0\n",
      "\n",
      "    # Luz's double clustering approach\n",
      "    for i in range(len(adatas)):\n",
      "\n",
      "        print(\"processing \" + str(i) + \"/\" + str(len(adatas)))\n",
      "        curscr = scr.Scrublet(adatas[i].X)\n",
      "        doublet_scores, predicted_doublets = curscr.scrub_doublets()\n",
      "        adatas[i].obs['scrublet_score'] = doublet_scores\n",
      "        adatas[i].obs['scrublet_pred'] = predicted_doublets\n",
      "        #overcluster prep. run turbo basic scanpy pipeline\n",
      "        sc.pp.normalize_per_cell(adatas[i], counts_per_cell_after=1e4)\n",
      "        sc.pp.log1p(adatas[i])\n",
      "        sc.pp.highly_variable_genes(adatas[i], min_mean=0.0125, max_mean=3, min_disp=0.5)\n",
      "        adatas[i] = adatas[i][:, adatas[i].var['highly_variable']]\n",
      "        sc.pp.scale(adatas[i], max_value=10)\n",
      "        sc.tl.pca(adatas[i], svd_solver='arpack')\n",
      "        sc.pp.neighbors(adatas[i])\n",
      "        #overclustering proper - do basic clustering first, then cluster each cluster\n",
      "        sc.tl.leiden(adatas[i])\n",
      "        adatas[i].obs['leiden'] = [str(i) for i in adatas[i].obs['leiden']]\n",
      "        for clus in np.unique(adatas[i].obs['leiden']):\n",
      "            adata_sub = adatas[i][adatas[i].obs['leiden']==clus].copy()\n",
      "            sc.tl.leiden(adata_sub)\n",
      "            adata_sub.obs['leiden'] = [clus+','+i for i in adata_sub.obs['leiden']]\n",
      "            adatas[i].obs.loc[adata_sub.obs_names,'leiden'] = adata_sub.obs['leiden']\n",
      "\n",
      "        #compute the cluster scores - the median of Scrublet scores per overclustered cluster\n",
      "        for clus in np.unique(adatas[i].obs['leiden']):\n",
      "            adatas[i].obs.loc[adatas[i].obs['leiden']==clus, 'scrublet_cluster_score'] = \\\n",
      "                np.median(adatas[i].obs.loc[adatas[i].obs['leiden']==clus, 'scrublet_score'])\n",
      "        #now compute doublet p-values. figure out the median and mad (from above-median values) for the distribution\n",
      "        med = np.median(adatas[i].obs['scrublet_cluster_score'])\n",
      "        mask = adatas[i].obs['scrublet_cluster_score']>med\n",
      "        mad = np.median(adatas[i].obs['scrublet_cluster_score'][mask]-med)\n",
      "        #let's do a one-sided test. the Bertie write-up does not address this but it makes sense\n",
      "        zscores = (adatas[i].obs['scrublet_cluster_score'].values - med) / (1.4826 * mad)\n",
      "        #adatas[i].obs['zscore'] = zscores\n",
      "        pvals = 1-scipy.stats.norm.cdf(zscores)\n",
      "        #adatas[i].obs['bh_pval'] = bh(pvals)\n",
      "        pvals = bonf(pvals)\n",
      "        adatas[i].obs['scrublet_local_pred'] = pvals < bonf_threshold\n",
      "        map = [tmap[s] for s in adatas[i].obs_names]\n",
      "        print(\"annoying values\")\n",
      "        for j in range(len(adatas[i].obs_names)):\n",
      "            adata.obs[\"scrublet_pred\" ][map[j]] = adatas[i].obs[\"scrublet_pred\"][j]\n",
      "            adata.obs[\"scrublet_local_pred\"][map[j]] = adatas[i].obs[\"scrublet_local_pred\"][j]\n",
      "            adata.obs[\"scrublet_cluster_score\"][map[j]] =  adatas[i].obs[\"scrublet_cluster_score\"][j]\n",
      "            adata.obs[\"scrublet_score\"][map[j]] =  pvals[j]\n",
      "\n",
      "    adata.obs[\"scrublet_cluster_score\"] = pd.to_numeric(adata.obs[\"scrublet_cluster_score\"], downcast='float')\n",
      "    adata.obs[\"scrublet_score\"] = pd.to_numeric(adata.obs[\"scrublet_score\"], downcast='float')\n",
      "\n",
      "    if add_cell_filter is not None:\n",
      "        curflt = np.zeros(adata.obs.shape[0], dtype=bool) \n",
      "        for k,v in add_cell_filter.items():\n",
      "            if k == \"max_percent_mito\":\n",
      "                print(str(sum(adata.obs['percent_mito'] > v)) + \" cells filtered by mito threshold\")\n",
      "                curflt |= adata.obs['percent_mito'] > v\n",
      "            else:\n",
      "                print(str(sum(adata.obs[k] != v)) + \" cells filtered by \" + k + \" == \" + str(v) + \" criterion\")\n",
      "                curflt |= (adata.obs[k] != v)\n",
      "        adata.obs[obskey_cellfilter] = curflt\n",
      "    return adata\n",
      "\n",
      "\u001b[31;43;1mExecution:\u001b[0m\n",
      "spliting data using attribute sample_names\n",
      "processing 0/19\n",
      "Preprocessing...\n",
      "Simulating doublets...\n",
      "Embedding transcriptomes using PCA...\n",
      "Calculating doublet scores...\n",
      "Automatically set threshold at doublet score = 0.73\n",
      "Detected doublet rate = 0.3%\n",
      "Estimated detectable doublet fraction = 9.2%\n",
      "Overall doublet rate:\n",
      "\tExpected   = 10.0%\n",
      "\tEstimated  = 2.9%\n",
      "Elapsed time: 16.1 seconds\n",
      "annoying values\n",
      "processing 1/19\n",
      "Preprocessing...\n",
      "Simulating doublets...\n",
      "Embedding transcriptomes using PCA...\n",
      "Calculating doublet scores...\n",
      "Automatically set threshold at doublet score = 0.72\n",
      "Detected doublet rate = 0.3%\n",
      "Estimated detectable doublet fraction = 5.6%\n",
      "Overall doublet rate:\n",
      "\tExpected   = 10.0%\n",
      "\tEstimated  = 5.5%\n",
      "Elapsed time: 4.7 seconds\n",
      "annoying values\n",
      "processing 2/19\n",
      "Preprocessing...\n",
      "Simulating doublets...\n",
      "Embedding transcriptomes using PCA...\n",
      "Calculating doublet scores...\n",
      "Automatically set threshold at doublet score = 0.23\n",
      "Detected doublet rate = 8.7%\n",
      "Estimated detectable doublet fraction = 74.8%\n",
      "Overall doublet rate:\n",
      "\tExpected   = 10.0%\n",
      "\tEstimated  = 11.7%\n",
      "Elapsed time: 2.9 seconds\n",
      "annoying values\n",
      "processing 3/19\n",
      "Preprocessing...\n",
      "Simulating doublets...\n",
      "Embedding transcriptomes using PCA...\n",
      "Calculating doublet scores...\n",
      "Automatically set threshold at doublet score = 0.31\n",
      "Detected doublet rate = 4.8%\n",
      "Estimated detectable doublet fraction = 68.2%\n",
      "Overall doublet rate:\n",
      "\tExpected   = 10.0%\n",
      "\tEstimated  = 7.0%\n",
      "Elapsed time: 6.0 seconds\n",
      "annoying values\n",
      "processing 4/19\n",
      "Preprocessing...\n",
      "Simulating doublets...\n",
      "Embedding transcriptomes using PCA...\n",
      "Calculating doublet scores...\n",
      "Automatically set threshold at doublet score = 0.61\n",
      "Detected doublet rate = 0.8%\n",
      "Estimated detectable doublet fraction = 18.6%\n",
      "Overall doublet rate:\n",
      "\tExpected   = 10.0%\n",
      "\tEstimated  = 4.3%\n",
      "Elapsed time: 5.8 seconds\n",
      "annoying values\n",
      "processing 5/19\n",
      "Preprocessing...\n",
      "Simulating doublets...\n",
      "Embedding transcriptomes using PCA...\n",
      "Calculating doublet scores...\n",
      "Automatically set threshold at doublet score = 0.30\n",
      "Detected doublet rate = 5.1%\n",
      "Estimated detectable doublet fraction = 56.9%\n",
      "Overall doublet rate:\n",
      "\tExpected   = 10.0%\n",
      "\tEstimated  = 9.0%\n",
      "Elapsed time: 8.7 seconds\n",
      "annoying values\n",
      "processing 6/19\n",
      "Preprocessing...\n",
      "Simulating doublets...\n",
      "Embedding transcriptomes using PCA...\n",
      "Calculating doublet scores...\n",
      "Automatically set threshold at doublet score = 0.31\n",
      "Detected doublet rate = 5.9%\n",
      "Estimated detectable doublet fraction = 50.6%\n",
      "Overall doublet rate:\n",
      "\tExpected   = 10.0%\n",
      "\tEstimated  = 11.6%\n",
      "Elapsed time: 11.7 seconds\n",
      "annoying values\n",
      "processing 7/19\n",
      "Preprocessing...\n",
      "Simulating doublets...\n",
      "Embedding transcriptomes using PCA...\n",
      "Calculating doublet scores...\n",
      "Automatically set threshold at doublet score = 0.31\n",
      "Detected doublet rate = 1.5%\n",
      "Estimated detectable doublet fraction = 45.5%\n",
      "Overall doublet rate:\n",
      "\tExpected   = 10.0%\n",
      "\tEstimated  = 3.4%\n",
      "Elapsed time: 0.4 seconds\n",
      "annoying values\n",
      "processing 8/19\n",
      "Preprocessing...\n",
      "Simulating doublets...\n",
      "Embedding transcriptomes using PCA...\n",
      "Calculating doublet scores...\n",
      "Automatically set threshold at doublet score = 0.36\n",
      "Detected doublet rate = 2.7%\n",
      "Estimated detectable doublet fraction = 52.4%\n",
      "Overall doublet rate:\n",
      "\tExpected   = 10.0%\n",
      "\tEstimated  = 5.2%\n",
      "Elapsed time: 4.5 seconds\n",
      "annoying values\n",
      "processing 9/19\n",
      "Preprocessing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating doublets...\n",
      "Embedding transcriptomes using PCA...\n",
      "Calculating doublet scores...\n",
      "Automatically set threshold at doublet score = 0.42\n",
      "Detected doublet rate = 2.1%\n",
      "Estimated detectable doublet fraction = 46.9%\n",
      "Overall doublet rate:\n",
      "\tExpected   = 10.0%\n",
      "\tEstimated  = 4.4%\n",
      "Elapsed time: 2.6 seconds\n",
      "annoying values\n",
      "processing 10/19\n",
      "Preprocessing...\n",
      "Simulating doublets...\n",
      "Embedding transcriptomes using PCA...\n",
      "Calculating doublet scores...\n",
      "Automatically set threshold at doublet score = 0.28\n",
      "Detected doublet rate = 3.7%\n",
      "Estimated detectable doublet fraction = 63.0%\n",
      "Overall doublet rate:\n",
      "\tExpected   = 10.0%\n",
      "\tEstimated  = 5.9%\n",
      "Elapsed time: 1.0 seconds\n",
      "annoying values\n",
      "processing 11/19\n",
      "Preprocessing...\n",
      "Simulating doublets...\n",
      "Embedding transcriptomes using PCA...\n",
      "Calculating doublet scores...\n",
      "Automatically set threshold at doublet score = 0.29\n",
      "Detected doublet rate = 11.0%\n",
      "Estimated detectable doublet fraction = 62.0%\n",
      "Overall doublet rate:\n",
      "\tExpected   = 10.0%\n",
      "\tEstimated  = 17.8%\n",
      "Elapsed time: 12.2 seconds\n",
      "annoying values\n",
      "processing 12/19\n",
      "Preprocessing...\n",
      "Simulating doublets...\n",
      "Embedding transcriptomes using PCA...\n",
      "Calculating doublet scores...\n",
      "Automatically set threshold at doublet score = 0.45\n",
      "Detected doublet rate = 2.2%\n",
      "Estimated detectable doublet fraction = 42.7%\n",
      "Overall doublet rate:\n",
      "\tExpected   = 10.0%\n",
      "\tEstimated  = 5.2%\n",
      "Elapsed time: 3.5 seconds\n",
      "annoying values\n",
      "processing 13/19\n",
      "Preprocessing...\n",
      "Simulating doublets...\n",
      "Embedding transcriptomes using PCA...\n",
      "Calculating doublet scores...\n",
      "Automatically set threshold at doublet score = 0.51\n",
      "Detected doublet rate = 0.2%\n",
      "Estimated detectable doublet fraction = 21.6%\n",
      "Overall doublet rate:\n",
      "\tExpected   = 10.0%\n",
      "\tEstimated  = 0.9%\n",
      "Elapsed time: 0.4 seconds\n",
      "annoying values\n",
      "processing 14/19\n",
      "Preprocessing...\n",
      "Simulating doublets...\n",
      "Embedding transcriptomes using PCA...\n",
      "Calculating doublet scores...\n",
      "Automatically set threshold at doublet score = 0.32\n",
      "Detected doublet rate = 2.2%\n",
      "Estimated detectable doublet fraction = 58.8%\n",
      "Overall doublet rate:\n",
      "\tExpected   = 10.0%\n",
      "\tEstimated  = 3.8%\n",
      "Elapsed time: 0.8 seconds\n",
      "annoying values\n",
      "processing 15/19\n",
      "Preprocessing...\n",
      "Simulating doublets...\n",
      "Embedding transcriptomes using PCA...\n",
      "Calculating doublet scores...\n",
      "Automatically set threshold at doublet score = 0.53\n",
      "Detected doublet rate = 1.0%\n",
      "Estimated detectable doublet fraction = 24.7%\n",
      "Overall doublet rate:\n",
      "\tExpected   = 10.0%\n",
      "\tEstimated  = 4.0%\n",
      "Elapsed time: 0.7 seconds\n",
      "annoying values\n",
      "processing 16/19\n",
      "Preprocessing...\n",
      "Simulating doublets...\n",
      "Embedding transcriptomes using PCA...\n",
      "Calculating doublet scores...\n",
      "Automatically set threshold at doublet score = 0.31\n",
      "Detected doublet rate = 4.9%\n",
      "Estimated detectable doublet fraction = 55.0%\n",
      "Overall doublet rate:\n",
      "\tExpected   = 10.0%\n",
      "\tEstimated  = 8.9%\n",
      "Elapsed time: 9.1 seconds\n",
      "annoying values\n",
      "processing 17/19\n",
      "Preprocessing...\n",
      "Simulating doublets...\n",
      "Embedding transcriptomes using PCA...\n",
      "Calculating doublet scores...\n",
      "Automatically set threshold at doublet score = 0.45\n",
      "Detected doublet rate = 1.4%\n",
      "Estimated detectable doublet fraction = 45.5%\n",
      "Overall doublet rate:\n",
      "\tExpected   = 10.0%\n",
      "\tEstimated  = 3.0%\n",
      "Elapsed time: 0.8 seconds\n",
      "annoying values\n",
      "processing 18/19\n",
      "Preprocessing...\n",
      "Simulating doublets...\n",
      "Embedding transcriptomes using PCA...\n",
      "Calculating doublet scores...\n",
      "Automatically set threshold at doublet score = 0.58\n",
      "Detected doublet rate = 0.9%\n",
      "Estimated detectable doublet fraction = 23.1%\n",
      "Overall doublet rate:\n",
      "\tExpected   = 10.0%\n",
      "\tEstimated  = 3.7%\n",
      "Elapsed time: 1.3 seconds\n",
      "annoying values\n",
      "7200 cells filtered by mito threshold\n",
      "3565 cells filtered by scrublet_local_pred == False criterion\n"
     ]
    }
   ],
   "source": [
    "obskey_filteredcells = \"filtered_cells\"\n",
    "adata = lpy.scrub(adata, \"sample_names\",obskey_cellfilter= obskey_filteredcells ,doinspect=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35;46;1mPrepare Data for Scvi/TotalVi\u001b[0m\u001b[34m\n",
      "def scvi_prepare(anndatapath, field, cellfilter = None, nbgenes = 5000, genes_to_filter= None, use_ccfilter_prefix=None, citeseqkey = \"protein_expression\", use_raw_slot_instead =None, min_cell_threshold= 0, doinspect=False):\n",
      "    if doinspect is True: print(\"\\033[35;46;1mPrepare Data for Scvi/TotalVi\\033[0m\\033[34m\"); print(inspect.getsource(scvi_prepare));print(\"\\033[31;43;1mExecution:\\033[0m\")\n",
      "    if use_ccfilter_prefix is not None :\n",
      "        genes_to_filter = [use_ccfilter_prefix + x for x in [\"HMGB2\",\"CDK1\",\"NUSAP1\",\"UBE2C\",\"BIRC5\",\"TPX2\",\"TOP2A\",\"NDC80\",\"CKS2\",\"NUF2\",\"CKS1B\",\"MKI67\",\"TMPO\",\"CENPF\",\"TACC3\",\"FAM64A\",\"SMC4\",\"CCNB2\",\"CKAP2L\",\"CKAP2\",\"AURKB\",\"BUB1\",\"KIF11\",\"ANP32E\",\"TUBB4B\",\"GTSE1\",\"KIF20B\",\"HJURP\",\"CDCA3\",\"HN1\",\"CDC20\",\"TTK\",\"CDC25C\",\"KIF2C\",\"RANGAP1\",\"NCAPD2\",\"DLGAP5\",\"CDCA2\",\"CDCA8\",\"ECT2\",\"KIF23\",\"HMMR\",\"AURKA\",\"PSRC1\",\"ANLN\",\"LBR\",\"CKAP5\",\"CENPE\",\"CTCF\",\"NEK2\",\"G2E3\",\"GAS2L3\",\"CBX5\",\"CENPA\",\"MCM5\",\"PCNA\",\"TYMS\",\"FEN1\",\"MCM2\",\"MCM4\",\"RRM1\",\"UNG\",\"GINS2\",\"MCM6\",\"CDCA7\",\"DTL\",\"PRIM1\",\"UHRF1\",\"MLF1IP\",\"HELLS\",\"RFC2\",\"RPA2\",\"NASP\",\"RAD51AP1\",\"GMNN\",\"WDR76\",\"SLBP\",\"CCNE2\",\"UBR7\",\"POLD3\",\"MSH2\",\"ATAD2\",\"RAD51\",\"RRM2\",\"CDC45\",\"CDC6\",\"EXO1\",\"TIPIN\",\"DSCC1\",\"BLM\",\"CASP8AP2\",\"USP1\",\"CLSPN\",\"POLA1\",\"CHAF1B\",\"BRIP1\",\"E2F8\"]]\n",
      "    if (isinstance(anndatapath, str)): cite = anndata.read_h5ad(anndatapath)\n",
      "    else: cite = anndatapath\n",
      "    if use_raw_slot_instead is None: # default behavior, check for the existance of the raw slot\n",
      "        use_raw_slot_instead = cite.raw is not None\n",
      "    \n",
      "    split = splitAnnData(cite, field, entryfilter= cellfilter, getnames = True, use_raw_slot_instead= use_raw_slot_instead,min_cell_threshold=min_cell_threshold)\n",
      "    if genes_to_filter is not None:\n",
      "        for i in range(len(split[\"datalist\"])):\n",
      "            split[\"datalist\"][i] = split[\"datalist\"][i][:, [g not in genes_to_filter for g in split[\"datalist\"][i].var_names]]\n",
      "    \n",
      "    \n",
      "\n",
      "    dataset = GeneExpressionDataset()\n",
      "    if citeseqkey in cite.obsm.keys():\n",
      "        if citeseqkey is not \"protein_expression\":\n",
      "            for i in range(len(split[\"datalist\"])):\n",
      "                split[\"datalist\"][i].obsm[\"protein_expression\"] = split[\"datalist\"][i].obsm[citeseqkey]\n",
      "        dataset.populate_from_datasets([AnnDatasetFromAnnData(ad=s,cell_measurements_col_mappings={\"protein_expression\":\"protein_names\"})   for s in split[\"datalist\"] ])\n",
      "    else:\n",
      "        dataset.populate_from_datasets([AnnDatasetFromAnnData(ad=s)   for s in split[\"datalist\"] ])\n",
      "\n",
      "    if nbgenes != 0: # nbgenes == 0 is a flag for the default behavior, which is to include all the genes\n",
      "        dataset.subsample_genes(nbgenes)\n",
      "    return{\"dataset\":dataset , \"names\" : split[\"ordered\"]}\n",
      "\n",
      "\u001b[31;43;1mExecution:\u001b[0m\n",
      "[2020-11-08 17:27:56,391] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-11-08 17:27:57,023] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 17:27:57,025] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 17:27:58,332] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 17:27:58,539] INFO - scvi.dataset.dataset | Downsampled from 6008 to 6008 cells\n",
      "[2020-11-08 17:27:58,637] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-11-08 17:27:59,112] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 17:27:59,114] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 17:28:00,069] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 17:28:00,218] INFO - scvi.dataset.dataset | Downsampled from 4310 to 4310 cells\n",
      "[2020-11-08 17:28:00,274] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-11-08 17:28:00,735] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 17:28:00,737] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 17:28:01,572] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 17:28:01,722] INFO - scvi.dataset.dataset | Downsampled from 4344 to 4344 cells\n",
      "[2020-11-08 17:28:01,813] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-11-08 17:28:02,468] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 17:28:02,480] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 17:28:03,770] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 17:28:03,989] INFO - scvi.dataset.dataset | Downsampled from 6353 to 6353 cells\n",
      "[2020-11-08 17:28:04,105] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-11-08 17:28:04,747] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 17:28:04,749] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 17:28:06,064] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 17:28:06,274] INFO - scvi.dataset.dataset | Downsampled from 6111 to 6111 cells\n",
      "[2020-11-08 17:28:06,379] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-11-08 17:28:07,187] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 17:28:07,190] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 17:28:08,770] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 17:28:09,043] INFO - scvi.dataset.dataset | Downsampled from 7975 to 7975 cells\n",
      "[2020-11-08 17:28:09,175] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-11-08 17:28:10,204] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 17:28:10,206] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 17:28:12,276] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 17:28:12,633] INFO - scvi.dataset.dataset | Downsampled from 10440 to 10440 cells\n",
      "[2020-11-08 17:28:12,689] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-11-08 17:28:12,817] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 17:28:12,819] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 17:28:12,964] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 17:28:12,986] INFO - scvi.dataset.dataset | Downsampled from 633 to 633 cells\n",
      "[2020-11-08 17:28:13,049] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-11-08 17:28:13,354] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 17:28:13,356] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 17:28:13,954] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 17:28:14,037] INFO - scvi.dataset.dataset | Downsampled from 2406 to 2406 cells\n",
      "[2020-11-08 17:28:14,087] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-11-08 17:28:14,456] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 17:28:14,458] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 17:28:15,128] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 17:28:15,243] INFO - scvi.dataset.dataset | Downsampled from 3349 to 3349 cells\n",
      "[2020-11-08 17:28:15,281] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-11-08 17:28:15,509] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 17:28:15,511] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 17:28:15,887] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 17:28:15,950] INFO - scvi.dataset.dataset | Downsampled from 1801 to 1801 cells\n",
      "[2020-11-08 17:28:16,024] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-11-08 17:28:17,247] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-11-08 17:28:17,249] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 17:28:23,198] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 17:28:23,652] INFO - scvi.dataset.dataset | Downsampled from 12681 to 12681 cells\n",
      "[2020-11-08 17:28:23,772] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-11-08 17:28:24,074] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 17:28:24,077] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 17:28:24,583] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 17:28:24,665] INFO - scvi.dataset.dataset | Downsampled from 2363 to 2363 cells\n",
      "[2020-11-08 17:28:24,685] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-11-08 17:28:24,819] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 17:28:24,821] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 17:28:24,938] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 17:28:24,960] INFO - scvi.dataset.dataset | Downsampled from 603 to 603 cells\n",
      "[2020-11-08 17:28:24,987] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-11-08 17:28:25,192] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 17:28:25,194] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 17:28:25,502] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 17:28:25,554] INFO - scvi.dataset.dataset | Downsampled from 1495 to 1495 cells\n",
      "[2020-11-08 17:28:25,580] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-11-08 17:28:25,728] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 17:28:25,730] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 17:28:25,939] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 17:28:25,970] INFO - scvi.dataset.dataset | Downsampled from 879 to 879 cells\n",
      "[2020-11-08 17:28:26,054] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-11-08 17:28:26,919] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 17:28:26,921] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 17:28:30,485] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 17:28:30,818] INFO - scvi.dataset.dataset | Downsampled from 8467 to 8467 cells\n",
      "[2020-11-08 17:28:30,897] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-11-08 17:28:31,113] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 17:28:31,115] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 17:28:31,432] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 17:28:31,486] INFO - scvi.dataset.dataset | Downsampled from 1552 to 1552 cells\n",
      "[2020-11-08 17:28:31,525] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-11-08 17:28:31,732] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 17:28:31,734] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 17:28:32,077] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 17:28:32,126] INFO - scvi.dataset.dataset | Downsampled from 1408 to 1408 cells\n",
      "[2020-11-08 17:28:32,138] INFO - scvi.dataset.dataset | Merging datasets. Input objects are modified in place.\n",
      "[2020-11-08 17:28:32,140] INFO - scvi.dataset.dataset | Gene names and cell measurement names are assumed to have a non-null intersection between datasets.\n",
      "[2020-11-08 17:28:32,387] INFO - scvi.dataset.dataset | Keeping 19208 genes\n",
      "[2020-11-08 17:28:35,005] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 17:28:35,483] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 17:28:35,485] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 17:28:36,430] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 17:28:36,783] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 17:28:36,785] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 17:28:37,644] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 17:28:37,973] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 17:28:37,976] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 17:28:39,467] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 17:28:39,974] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 17:28:39,976] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 17:28:41,298] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 17:28:41,835] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 17:28:41,837] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 17:28:44,162] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 17:28:44,811] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 17:28:44,813] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 17:28:47,952] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 17:28:48,738] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 17:28:48,740] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 17:28:48,897] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 17:28:48,962] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 17:28:48,965] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 17:28:49,555] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 17:28:49,757] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 17:28:49,759] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 17:28:50,441] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 17:28:50,713] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 17:28:50,715] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 17:28:51,101] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 17:28:51,248] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 17:28:51,250] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 17:28:54,491] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 17:28:55,520] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 17:28:55,522] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 17:28:56,035] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 17:28:56,229] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 17:28:56,231] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 17:28:56,361] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 17:28:56,422] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 17:28:56,424] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 17:28:56,744] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 17:28:56,870] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 17:28:56,872] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 17:28:57,084] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 17:28:57,166] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-11-08 17:28:57,168] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 17:28:58,868] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 17:28:59,537] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 17:28:59,539] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 17:28:59,868] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 17:28:59,993] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 17:28:59,995] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 17:29:00,337] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 17:29:00,494] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 17:29:00,496] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 17:29:28,318] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 17:29:28,355] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 17:29:31,294] INFO - scvi.dataset.dataset | extracting highly variable genes using seurat_v3 flavor\n",
      "[2020-11-08 17:30:07,254] INFO - scvi.dataset.dataset | Downsampling from 19208 to 5000 genes\n",
      "[2020-11-08 17:30:13,721] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 17:30:14,865] INFO - scvi.dataset.dataset | Filtering non-expressing cells.\n",
      "[2020-11-08 17:30:19,853] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 17:30:20,530] INFO - scvi.dataset.dataset | Downsampled from 83178 to 83178 cells\n"
     ]
    }
   ],
   "source": [
    "# prepare data for scvi, subset out doublets and populate scvi objects \n",
    "sp = lpy.scvi_prepare(adata, \"sample_names\", adata.obs[obskey_filteredcells] == False, use_ccfilter_prefix=\"\", doinspect=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35;46;1mRun scvi\u001b[0m\u001b[34m\n",
      "def runSCVI(dataset, nbstep = 500, n_latent = 64, doinspect= False):\n",
      "    if doinspect is True: print(\"\\033[35;46;1mRun scvi\\033[0m\\033[34m\"); print(inspect.getsource(runSCVI));print(\"\\033[31;43;1mExecution:\\033[0m\")\n",
      "    vae = VAE(dataset.nb_genes, n_batch= dataset.n_batches, n_labels= dataset.n_labels, n_latent = n_latent)\n",
      "    trainer = UnsupervisedTrainer(vae, dataset, train_size=0.9, frequency=5, use_cuda=True)\n",
      "    trainer.train(n_epochs=nbstep)\n",
      "    full = trainer.create_posterior(trainer.model, dataset, indices=np.arange(len(dataset)))\n",
      "    return(full.sequential().get_latent()[0])\n",
      "\n",
      "\u001b[31;43;1mExecution:\u001b[0m\n",
      "[2020-11-08 17:30:33,636] INFO - scvi.inference.inference | KL warmup for 400 epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "073b3ee511404d31aded68096c9233e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training', max=500.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# run scvi\n",
    "latent = lpy.runSCVI(sp[\"dataset\"], doinspect=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35;46;1mCompute Clusters and Reduces representations\u001b[0m\u001b[34m\n",
      "def insertLatent(adata, latent , latent_key= \"latent\", umap_key= \"X_umap\", tsne_key = \"X_tsne\", leiden_key = \"leiden\", rename_cluster_key= None,cellfilter = None, cellnames =None, leiden_resolution=1.0,doinspect=False):\n",
      "    if doinspect is True: print(\"\\033[35;46;1mCompute Clusters and Reduces representations\\033[0m\\033[34m\"); print(inspect.getsource(insertLatent));print(\"\\033[31;43;1mExecution:\\033[0m\")\n",
      "\n",
      "\n",
      "    if cellnames is None:\n",
      "        #order of full must match\n",
      "        assert latent.shape[0] == len(adata.obs_names),  \"cell names need for be provided if size of latent mismatches adata\"\n",
      "        map = range(len(adata.obs_names))\n",
      "        if latent_key is not None:\n",
      "            adata.obsm[latent_key] = latent\n",
      "    else:\n",
      "        print(\"defining permutation\")\n",
      "        dalist = list(adata.obs_names)\n",
      "        tmap = {}\n",
      "        for i in range(len(adata.obs_names)):\n",
      "            tmap.update( {adata.obs_names[i] : i})\n",
      "# for i in range(len(adata.obs_names))}\n",
      "        map = [tmap[s] for s in cellnames]\n",
      "        if latent_key is not None:\n",
      "            print(\"Inserting Latent coords\")\n",
      "            adata.obsm[latent_key] = np.zeros( (len(adata.obs_names), latent.shape[1]) )\n",
      "            for i in range(len(map)):\n",
      "                adata.obsm[latent_key][map[i],:] = latent[i,:]\n",
      "    \n",
      "    if umap_key is not None:\n",
      "        import umap.umap_ as umap\n",
      "        print(\"computing UMAP\")\n",
      "        tumap = umap.UMAP(spread=2).fit_transform(latent)\n",
      "        adata.obsm[umap_key] = np.zeros( ( len(adata.obs_names), 2 ) )\n",
      "\n",
      "        print(\"Inserting Umap coords\")\n",
      "        for i in range(len(map)):\n",
      "            adata.obsm[umap_key][map[i],:] = tumap[i,:]\n",
      "\n",
      "    if tsne_key is not None:\n",
      "        import scanpy as sc\n",
      "        print(\"computing Tsne\")\n",
      "        adata_latent = sc.AnnData(latent)\n",
      "\n",
      "        sc.tl.tsne(adata_latent, use_rep='X', n_jobs=8)\n",
      "        adata.obsm[tsne_key] = np.zeros( ( len(adata.obs_names), 2 ) )\n",
      "\n",
      "        print(\"Inserting Tsne coords\")\n",
      "        for i in range(len(map)):\n",
      "            adata.obsm[tsne_key][map[i],:] = adata_latent.obsm[\"X_tsne\"][i,:]\n",
      "\n",
      "    if leiden_key is not None:\n",
      "        import scanpy as sc\n",
      "        adata_latent = sc.AnnData(latent)\n",
      "        print(\"Finding clusters\")\n",
      "        sc.pp.neighbors(adata_latent, use_rep='X', n_neighbors=30, metric='minkowski')\n",
      "        sc.tl.leiden(adata_latent, resolution=leiden_resolution)\n",
      "        adata.obs[leiden_key] = \"filtered\"\n",
      "        #if rename_cluster_key is not None:\n",
      "        #    print(\"Renaming to do...\")\n",
      "        #    for i\n",
      "        #else\n",
      "        #    ctnames = range(200)\n",
      "        \n",
      "        print(\"Inserting Cluster Id\")\n",
      "        for i in range(len(map)):\n",
      "            adata.obs[leiden_key][map[i]] = adata_latent.obs[\"leiden\"][i]\n",
      "    return adata\n",
      "\n",
      "\u001b[31;43;1mExecution:\u001b[0m\n",
      "defining permutation\n",
      "Inserting Latent coords\n",
      "computing UMAP\n",
      "Inserting Umap coords\n",
      "Finding clusters\n",
      "Inserting Cluster Id\n"
     ]
    }
   ],
   "source": [
    "# store latent variable back in th object, and produce UMAP coordinates and leiden clusters \n",
    "adata = lpy.insertLatent(adata,latent, \"scvi_sampl_cc\", \"X_umap_scvi_sampl_cc\", None, \"leiden_scvi_sampl_cc\", cellnames = sp[\"names\"], doinspect=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defining permutation\n",
      "Finding clusters\n",
      "Inserting Cluster Id\n"
     ]
    }
   ],
   "source": [
    "# additionnally produce leiden clusters with resolution 2\n",
    "adata = lpy.insertLatent(adata,latent, None, None, None, \"leidenres2_scvi_sampl_cc\", cellnames = sp[\"names\"], doinspect=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-11-08 18:26:33,550] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-11-08 18:26:34,296] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 18:26:34,298] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 18:26:35,536] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 18:26:35,651] INFO - scvi.dataset.dataset | Downsampled from 3349 to 3349 cells\n",
      "[2020-11-08 18:26:35,742] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-11-08 18:26:37,453] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 18:26:37,456] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 18:26:40,614] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 18:26:40,993] INFO - scvi.dataset.dataset | Downsampled from 8467 to 8467 cells\n",
      "[2020-11-08 18:26:41,120] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-11-08 18:26:41,937] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 18:26:41,939] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 18:26:43,987] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 18:26:44,264] INFO - scvi.dataset.dataset | Downsampled from 7975 to 7975 cells\n",
      "[2020-11-08 18:26:44,400] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-11-08 18:26:45,474] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 18:26:45,477] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 18:26:48,152] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 18:26:48,513] INFO - scvi.dataset.dataset | Downsampled from 10440 to 10440 cells\n",
      "[2020-11-08 18:26:48,560] INFO - scvi.dataset.dataset | Merging datasets. Input objects are modified in place.\n",
      "[2020-11-08 18:26:48,562] INFO - scvi.dataset.dataset | Gene names and cell measurement names are assumed to have a non-null intersection between datasets.\n",
      "[2020-11-08 18:26:48,615] INFO - scvi.dataset.dataset | Keeping 19208 genes\n",
      "[2020-11-08 18:26:49,309] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 18:26:49,578] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 18:26:49,580] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 18:26:52,328] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 18:26:52,997] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 18:26:52,999] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 18:26:54,685] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 18:26:55,320] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 18:26:55,322] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 18:26:57,943] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 18:26:58,757] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 18:26:58,760] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 18:27:08,020] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 18:27:08,045] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 18:27:09,185] INFO - scvi.dataset.dataset | extracting highly variable genes using seurat_v3 flavor\n",
      "[2020-11-08 18:27:22,085] INFO - scvi.dataset.dataset | Downsampling from 19208 to 5000 genes\n",
      "[2020-11-08 18:27:24,258] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 18:27:24,763] INFO - scvi.dataset.dataset | Filtering non-expressing cells.\n",
      "[2020-11-08 18:27:26,583] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 18:27:26,859] INFO - scvi.dataset.dataset | Downsampled from 30231 to 30231 cells\n",
      "[2020-11-08 18:27:28,017] INFO - scvi.inference.inference | KL warmup for 400 epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db303fb1e3b14e808b12c48d22767ca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training', max=500.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "defining permutation\n",
      "Inserting Latent coords\n",
      "computing UMAP\n",
      "Inserting Umap coords\n",
      "Finding clusters\n",
      "Inserting Cluster Id\n"
     ]
    }
   ],
   "source": [
    "# redo the 4 steps above for a cell subset for the organ donor A10 only\n",
    "sp = lpy.scvi_prepare(adata, \"sample_names\", (adata.obs[obskey_filteredcells] == False)&(adata.obs[\"DonorID\"] == \"A10\"), use_ccfilter_prefix=\"\")\n",
    "latent = lpy.runSCVI(sp[\"dataset\"])\n",
    "adata = lpy.insertLatent(adata,latent, \"A10_sampl_cc\", \"X_umap_A10_sampl_cc\", None, \"leiden_A10_sampl_cc\", cellnames = sp[\"names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-11-08 18:45:08,164] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-11-08 18:45:08,822] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 18:45:08,824] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 18:45:09,641] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 18:45:09,711] INFO - scvi.dataset.dataset | Downsampled from 1552 to 1552 cells\n",
      "[2020-11-08 18:45:09,841] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-11-08 18:45:10,008] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 18:45:10,010] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 18:45:10,311] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 18:45:10,367] INFO - scvi.dataset.dataset | Downsampled from 1495 to 1495 cells\n",
      "[2020-11-08 18:45:10,386] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-11-08 18:45:10,481] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 18:45:10,483] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 18:45:10,611] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 18:45:10,628] INFO - scvi.dataset.dataset | Downsampled from 633 to 633 cells\n",
      "[2020-11-08 18:45:10,652] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-11-08 18:45:10,951] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 18:45:10,953] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 18:45:11,835] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 18:45:11,924] INFO - scvi.dataset.dataset | Downsampled from 1801 to 1801 cells\n",
      "[2020-11-08 18:45:11,941] INFO - scvi.dataset.dataset | Merging datasets. Input objects are modified in place.\n",
      "[2020-11-08 18:45:11,944] INFO - scvi.dataset.dataset | Gene names and cell measurement names are assumed to have a non-null intersection between datasets.\n",
      "[2020-11-08 18:45:11,999] INFO - scvi.dataset.dataset | Keeping 19208 genes\n",
      "[2020-11-08 18:45:12,601] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 18:45:12,728] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 18:45:12,730] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 18:45:13,040] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 18:45:13,165] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 18:45:13,167] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 18:45:13,309] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 18:45:13,365] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 18:45:13,367] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 18:45:13,881] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 18:45:14,051] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 18:45:14,057] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 18:45:18,311] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-11-08 18:45:18,500] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-11-08 18:45:18,696] INFO - scvi.dataset.dataset | extracting highly variable genes using seurat_v3 flavor\n",
      "[2020-11-08 18:45:21,200] INFO - scvi.dataset.dataset | Downsampling from 19208 to 5000 genes\n",
      "[2020-11-08 18:45:21,562] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 18:45:21,648] INFO - scvi.dataset.dataset | Filtering non-expressing cells.\n",
      "[2020-11-08 18:45:21,984] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-11-08 18:45:22,023] INFO - scvi.dataset.dataset | Downsampled from 5481 to 5481 cells\n",
      "[2020-11-08 18:45:22,664] INFO - scvi.inference.inference | KL warmup for 400 epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb97a595a7fa4b20aa291d7090a1d2ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training', max=500.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "defining permutation\n",
      "Inserting Latent coords\n",
      "computing UMAP\n",
      "Inserting Umap coords\n",
      "Finding clusters\n",
      "Inserting Cluster Id\n"
     ]
    }
   ],
   "source": [
    "# redo the above for a cell subset from the organ donor A16 only\n",
    "sp = lpy.scvi_prepare(adata, \"sample_names\", (adata.obs[obskey_filteredcells] == False)&(adata.obs[\"DonorID\"] == \"A16\"), use_ccfilter_prefix=\"\")\n",
    "latent = lpy.runSCVI(sp[\"dataset\"])\n",
    "adata = lpy.insertLatent(adata,latent, \"A16_sampl_cc\", \"X_umap_A16_sampl_cc\", None, \"leiden_A16_sampl_cc\", cellnames = sp[\"names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "... storing 'SampleID' as categorical\n",
      "... storing 'DonorID' as categorical\n",
      "... storing 'BiopsyType' as categorical\n",
      "... storing 'Location' as categorical\n",
      "... storing 'Binary Stage' as categorical\n",
      "... storing 'Stage' as categorical\n",
      "... storing 'Day' as categorical\n",
      "... storing 'Women age' as categorical\n",
      "... storing '10x kit' as categorical\n",
      "... storing 'Treatment' as categorical\n",
      "... storing 'leiden_scvi_sampl_cc' as categorical\n",
      "... storing 'leidenres2_scvi_sampl_cc' as categorical\n",
      "... storing 'leiden_A10_sampl_cc' as categorical\n",
      "... storing 'leiden_A16_sampl_cc' as categorical\n"
     ]
    }
   ],
   "source": [
    "# save object\n",
    "adata.write_h5ad(\"N1-integrated_donors.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
